{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c4109-4798-4bd8-946d-c5150e30b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "sc.settings.set_figure_params(dpi=100)\n",
    "print(sc.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e65ed-7ca5-47f7-a054-43a40f46d3c7",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f7404-79e3-46a7-b92e-2323879bf3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../external_data/db.ags.pkl', 'rb') as f: ags = pkl.load(f)\n",
    "with open('../external_data/db.tras.pkl', 'rb') as f: tras = pkl.load(f)\n",
    "with open('../external_data/db.trbs.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "with open('../external_data/db.paired_tcrs.pkl', 'rb') as f: paired_tcrs = pkl.load(f)\n",
    "ags, tras, trbs = pd.Series(ags), pd.Series(tras), pd.Series(trbs)\n",
    "print(len(ags)); print(len(tras)); print(len(trbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e0e3e-e868-49cd-b00e-592f7198651d",
   "metadata": {},
   "source": [
    "### Setup the Model Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3f7b5-c438-4a2e-9999-aa22e2541940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c28b5-4df6-4d52-8e1f-f3ec702659a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (recon_batch, recon_len), mu, logvar = model(data)\n",
    "        if loss_func == 1:\n",
    "            loss = loss_function1(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 2:\n",
    "            loss = loss_function2(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 3:\n",
    "            loss = loss_function3(recon_batch, recon_len, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "    \n",
    "def test(epoch, loss_func):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data[0].to(device)\n",
    "            (recon_batch, recon_len), mu, logvar = model(data)\n",
    "            if loss_func == 1:\n",
    "                test_loss += loss_function1(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 2:\n",
    "                test_loss += loss_function2(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 3:\n",
    "                test_loss += loss_function3(recon_batch, recon_len, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e52826-e589-4a4a-9fe7-f0a22c5f7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the key parameters\n",
    "init_embed_size = 50-1\n",
    "protein_len = 48\n",
    "init_kernel_size = 3\n",
    "init_cnn_filters = 256\n",
    "init_kernel_stride = 1\n",
    "init_kernel_padding = 1\n",
    "secn_cnn_filters = 256\n",
    "latent_dim = 32\n",
    "vocab = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "# we want the embedding output to be the vocab with the length to allow for reconstruction\n",
    "out_embed_size = len(vocab)\n",
    "n_nodes_len = 32\n",
    "\n",
    "# define the convolutional variational autoencoder\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # encoding\n",
    "        self.fc1 = nn.Conv1d(\n",
    "            in_channels=init_embed_size, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding,\n",
    "        )\n",
    "        self.fc2 = nn.Conv1d(\n",
    "            in_channels=init_cnn_filters, out_channels=secn_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        # variational sampling\n",
    "        self.fc31 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc32 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, secn_cnn_filters*protein_len)\n",
    "        # decoding\n",
    "        self.fc5 = nn.ConvTranspose1d(\n",
    "            in_channels=secn_cnn_filters, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc6 = nn.ConvTranspose1d(\n",
    "            in_channels=init_cnn_filters, out_channels=out_embed_size, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc7 = nn.Linear(init_cnn_filters*protein_len, n_nodes_len)\n",
    "        self.fc8 = nn.Linear(n_nodes_len, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x1 = nn.LeakyReLU()(self.fc1(x[:, :-1, :]))\n",
    "        x2 = nn.LeakyReLU()(self.fc2(x1))\n",
    "        x2_ = nn.Flatten()(x2)\n",
    "        return self.fc31(x2_), self.fc32(x2_)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x4 = nn.LeakyReLU()(self.fc4(z))\n",
    "        x4_ = x4.view(-1, secn_cnn_filters, protein_len)\n",
    "        x5 = nn.LeakyReLU()(self.fc5(x4_))\n",
    "        x5_ = nn.Flatten()(x5)\n",
    "        x6 = nn.Sigmoid()(self.fc6(x5))\n",
    "        return x6, self.fc8(nn.LeakyReLU()(self.fc7(x5_)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101da92b-4a27-4c82-963b-05eb60ce48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function1(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "def loss_function2(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return TSE\n",
    "def loss_function3(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return BCE + KLD + TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22ee1e-bbb1-4083-abe9-42f40bfb8ac8",
   "metadata": {},
   "source": [
    "### TRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea3052-c8c9-44e0-b0eb-2f10c945637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67adc6f-da6d-44a9-b6e4-a6b2afd41380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the trbs\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99dc45-11cc-4222-9a82-ab0b8e2da3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = ConvVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a6266-ce82-4d10-9ac2-aa5e9c13e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05da45-21cb-406b-acd8-fd26248a2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608ce87-5dfd-4a5e-b255-268fd16afb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef08e9e-3266-4f37-9790-c960d24f7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1eb4a2-2093-4a12-a819-6544551e97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "trbs_test = pd.Series(trbs.iloc[idxs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d3a01-c777-46cd-80cb-4c29793888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d616849-bd34-4dd0-bf96-252a12bed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average difference in sequence length\n",
    "fig, ax = plt.subplots(figsize=[1, 3]); ax.grid(False)\n",
    "sns.boxplot(y=pd.Series(recon_lens).apply(round) - trbs_test.apply(len), color='dodgerblue')\n",
    "ax.set(ylabel='Length difference')\n",
    "# examine the distribution to be sure\n",
    "fig, ax = plt.subplots(figsize=[1, 3]); ax.grid(False)\n",
    "sns.violinplot(y=pd.Series(recon_lens).apply(round) - trbs_test.apply(len), color='dodgerblue')\n",
    "ax.set(ylabel='Length difference')\n",
    "# provide statistics\n",
    "(pd.Series(recon_lens).apply(round) - trbs_test.apply(len)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac9e19-4453-40a6-b7d4-0bde041cafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae.trb.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f6f02-fd4f-4762-8f7d-c0a896be0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('../models/model.convvae.trb.torch', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89100157-fdde-4d59-bc21-bc961bae5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f7ed9-5001-43f9-8e60-b361d1ce7b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2)\n",
    "adata.obs_names = trbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb25b78-4b28-4e44-862d-b58177d42490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a06dca-f3a5-4d8a-875c-6b693d9a5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the current data\n",
    "adata.write('../outs/adata.trb.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4e4c4-b242-4995-bc56-920d1a80b762",
   "metadata": {},
   "source": [
    "##### Example sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63f06d-e22c-4571-b7b4-1f637bd88e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report statistics, averages\n",
    "means = adata.X.mean(0); means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01960061-8522-4461-8749-cde600821559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report statistics, standard deviations\n",
    "stds = adata.X.std(0); stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd7d37-8ee5-4b5d-99a0-edd844b591c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a566b-0a07-4a12-8ca9-a5174b9fd1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein\n",
    "# pick a random example to demonstrbte forward fluidity\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=5, replace=False)\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # plot the predicted vs. truth\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(true, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    # highlight any discrepancies\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred-true, xticklabels=0, yticklabels=1, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    print(true_seq, pred_seq, levenshtein(true_seq, pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399320-2f8b-4bd2-a2db-e3ca8a82092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as expected each are around 0 to 1 and thus we may sample similarly\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "z = torch.Tensor([np.random.normal(loc=0, scale=1, size=1)[0] for idx in range(32)]).to('cuda')\n",
    "tmp_out, tmp_len = model.decode(z)\n",
    "tmp_out = tmp_out.clone().detach().cpu().numpy()\n",
    "tmp_len = tmp_len.clone().detach().cpu().numpy()\n",
    "tmp_out = tmp_out[0]; tmp_len = round(tmp_len[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812521b7-5830-4f04-b8cb-b8fa99c0ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve the sequence with a deduction system\n",
    "fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "data = pd.DataFrame(tmp_out.T, columns=vocab).T\n",
    "sns.heatmap(data, xticklabels=0, yticklabels=1, cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b8cc4-60f1-4e35-b0bd-6077e5879591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=5, replace=False)\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = np.random.choice(trbs_test, size=1)[0]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # plot the predicted vs. truth\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(true, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    # highlight any discrepancies\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred-true, xticklabels=0, yticklabels=1, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    print(true_seq, pred_seq, levenshtein(true_seq, pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271508fe-39bf-4931-9d3c-40a4fc281258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=100, replace=False)\n",
    "# trbck distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(trbs_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92661f7e-50d2-4584-8a14-fa3a3421de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']),\\\n",
    "ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1f5cc-8335-45d8-96fe-b0750d6b9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate back to a sequence using the length\n",
    "curr_len = 48; targ_len = tmp_len\n",
    "# compute the x-coordinates of the original\n",
    "xp = np.arange(curr_len) / (curr_len - 1)\n",
    "x = np.arange(targ_len) / (targ_len - 1)\n",
    "# interpolate the results\n",
    "res = np.array([np.interp(x, xp, tmp_out[idx, :]) for idx in range(tmp_out.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38b069-f531-4016-9f4b-6b0e81745990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve the sequence with a deduction system\n",
    "fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "data = pd.DataFrame(res.T, columns=vocab).T\n",
    "sns.heatmap(data, xticklabels=0, yticklabels=1, cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81e7ae-abf4-4845-be68-14bb9a193363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reveal the sequence\n",
    "''.join(data.idxmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bd812-3e9c-4dfb-ad2f-116fa5f382d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an engine to derive sequence\n",
    "def engine(model, z):\n",
    "    # derive the embedding and length\n",
    "    tmp_out, tmp_len = model.decode(z)\n",
    "    tmp_out = tmp_out.clone().detach().cpu().numpy()\n",
    "    tmp_len = tmp_len.clone().detach().cpu().numpy()\n",
    "    tmp_out = tmp_out[0]; tmp_len = round(tmp_len[0][0])\n",
    "    \n",
    "    # interpolate back to a sequence using the length\n",
    "    curr_len = 48; targ_len = tmp_len\n",
    "    # compute the x-coordinates of the original\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, tmp_out[idx, :]) for idx in range(tmp_out.shape[0])])\n",
    "\n",
    "    # derive the sequence\n",
    "    data = pd.DataFrame(res.T, columns=vocab).T\n",
    "    return ''.join(data.idxmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a2337-caa1-48e3-9ca5-0798cb5f88f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide more examples\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "for _ in range(100):\n",
    "    z_in = [np.random.normal(loc=means[idx], scale=stds[idx], size=1)[0] for idx in range(32)]\n",
    "    z = torch.Tensor(z_in).to('cuda')\n",
    "    print(engine(model, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b0234-477f-4f40-b4b4-ed16660fa753",
   "metadata": {},
   "source": [
    "### TRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afeea57-16f6-4b2e-b761-cf5ca7109f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# embed all of our unique TRAs\n",
    "with open('../outs/map.tra_to_embed.extended.pkl', 'rb') as f: tra_to_embed = pkl.load(f)\n",
    "X_tras = torch.stack([x.to(torch.float32) for x in tras.map(tra_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_tras)), size=round(len(X_tras)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_tras)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_tras_train = X_tras[idxs_train]\n",
    "X_tras_test = X_tras[idxs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614f957-7542-4535-bd88-9b0ca370a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the tras\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_tras_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_tras_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd871ac-4f0b-4294-bf5b-1558ae094e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = ConvVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ad3a9-0623-42a3-a245-069e08cf2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8edf4-1164-4bc5-b339-cd6cf9b56cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05907903-a959-4422-b12a-950e466672c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9888c9-76ad-41bb-b3fa-564e77e1eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364f77a-b60a-4284-8037-71a4c7bb76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "tras_test = pd.Series(tras.iloc[idxs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32649c1b-6e13-4c9e-a613-d32d79272b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, tras_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, tras_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), tras_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, tras_test.apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a5fe7-f2bd-4ca6-81eb-2c69750087dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average difference in sequence length\n",
    "fig, ax = plt.subplots(figsize=[1, 3]); ax.grid(False)\n",
    "sns.boxplot(y=pd.Series(recon_lens).apply(round) - tras_test.apply(len), color='dodgerblue')\n",
    "ax.set(ylabel='Length difference')\n",
    "# examine the distribution to be sure\n",
    "fig, ax = plt.subplots(figsize=[1, 3]); ax.grid(False)\n",
    "sns.violinplot(y=pd.Series(recon_lens).apply(round) - tras_test.apply(len), color='dodgerblue')\n",
    "ax.set(ylabel='Length difference')\n",
    "# provide statistics\n",
    "(pd.Series(recon_lens).apply(round) - tras_test.apply(len)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0884b-dfad-4d46-86c8-a7b469594d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae.tra.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8807e9e-9a3d-404d-af91-d522c9d01a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model.load_state_dict(torch.load('../models/model.convvae.tra.torch', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a004a3d-47a9-4cd1-8d03-21a721fed295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_tras), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13421611-484d-4beb-b3b9-fb8e149b9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2\n",
    "adata.obs_names = tras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8393c-d62d-4b08-862e-5c9f4d37340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f7724-4a3e-4537-a445-e26382990a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the current data\n",
    "adata.write('../outs/adata.tra.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaea19a-894a-4917-9bc3-909c6caf9de2",
   "metadata": {},
   "source": [
    "##### Example sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e48f5-a2ee-47cf-9744-e44cddc40fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report statistics, averages\n",
    "means = adata.X.mean(0); means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb2062-5749-4baa-8f9b-87281ded59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report statistics, standard deviations\n",
    "stds = adata.X.std(0); stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944f025-1735-4898-b81d-27250ac46512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cdecb-5ee1-40cf-8350-652d8256b223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein\n",
    "# pick a random example to demonstrate forward fluidity\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(tras_test)), size=5, replace=False)\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = tras_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # plot the predicted vs. truth\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(true, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    # highlight any discrepancies\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred-true, xticklabels=0, yticklabels=1, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    print(true_seq, pred_seq, levenshtein(true_seq, pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6da83-5a14-42c7-baf7-3e0824397dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as expected each are around 0 to 1 and thus we may sample similarly\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "z = torch.Tensor([np.random.normal(loc=0, scale=1, size=1)[0] for idx in range(32)]).to('cuda')\n",
    "tmp_out, tmp_len = model.decode(z)\n",
    "tmp_out = tmp_out.clone().detach().cpu().numpy()\n",
    "tmp_len = tmp_len.clone().detach().cpu().numpy()\n",
    "tmp_out = tmp_out[0]; tmp_len = round(tmp_len[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efcded5-abb2-4893-b61e-4e4ba7de37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve the sequence with a deduction system\n",
    "fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "data = pd.DataFrame(tmp_out.T, columns=vocab).T\n",
    "sns.heatmap(data, xticklabels=0, yticklabels=1, cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79a8a3-4294-4bdd-bc9c-b50885f25746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(tras_test)), size=5, replace=False)\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = np.random.choice(tras_test, size=1)[0]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # plot the predicted vs. truth\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(true, xticklabels=0, yticklabels=1, cmap='PuBu')\n",
    "    # highlight any discrepancies\n",
    "    fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "    sns.heatmap(pred-true, xticklabels=0, yticklabels=1, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    print(true_seq, pred_seq, levenshtein(true_seq, pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81bc88-9e83-4cc2-a7d0-5753e9882fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(tras_test)), size=100, replace=False)\n",
    "# track distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = tras_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(tras_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19df0f8-6d41-4425-817a-a8dca5709990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']),\\\n",
    "ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf244f-67ba-45cf-9872-4d2122864baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate back to a sequence using the length\n",
    "curr_len = 48; targ_len = tmp_len\n",
    "# compute the x-coordinates of the original\n",
    "xp = np.arange(curr_len) / (curr_len - 1)\n",
    "x = np.arange(targ_len) / (targ_len - 1)\n",
    "# interpolate the results\n",
    "res = np.array([np.interp(x, xp, tmp_out[idx, :]) for idx in range(tmp_out.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ed653-ed9e-40e2-8dd9-fe7e4e2b623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve the sequence with a deduction system\n",
    "fig, ax = plt.subplots(figsize=[8, 4]); ax.grid(False)\n",
    "data = pd.DataFrame(res.T, columns=vocab).T\n",
    "sns.heatmap(data, xticklabels=0, yticklabels=1, cmap='PuBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b4cde-30f9-4559-ba81-5cbaaaa917ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reveal the sequence\n",
    "''.join(data.idxmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867dbcc6-0ae1-4361-af3c-c08a10bcd863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provide more examples\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "for _ in range(100):\n",
    "    z_in = [np.random.normal(loc=means[idx], scale=stds[idx], size=1)[0] for idx in range(32)]\n",
    "    z = torch.Tensor(z_in).to('cuda')\n",
    "    print(engine(model, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4006c70-84bb-4e2b-9f05-4c2356c3ec5c",
   "metadata": {},
   "source": [
    "### Time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a85f94-8c4a-4280-8cc0-b3ebb3b3b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "# load the model\n",
    "model.load_state_dict(torch.load('../models/model.convvae.trb.torch', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "import time\n",
    "# test for a batch size of 1000\n",
    "sizes = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 500000, int(1e6)]\n",
    "batch_size = 5000\n",
    "df_time = pd.DataFrame(columns=['size','batch_size','seed','duration'])\n",
    "# get the time for different input sizes\n",
    "for size in tqdm(sizes):\n",
    "    for seed in range(5):\n",
    "        if df_time.loc[(df_time['size'] == size) & (df_time['batch_size'] == batch_size) & (df_time['seed'] == seed)].shape[0] > 0:\n",
    "            continue\n",
    "        # randomly split the data into train and test\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        idxs = np.random.choice(range(len(X_trbs)), size=size, replace=True)\n",
    "        # loop through the number of TRBs\n",
    "        loader = DataLoader(dataset=TensorDataset(X_trbs[idxs]), batch_size=batch_size, shuffle=False)\n",
    "        # time the inference\n",
    "        start = time.time()\n",
    "        for data in loader:\n",
    "            data = data[0].to(device)\n",
    "            enc_out = model.encode(data)\n",
    "        duration = time.time() - start\n",
    "        df_time.loc[df_time.shape[0]] = size, batch_size, seed, duration\n",
    "# save the dataframe temporarily in case of crash in the next size\n",
    "df_time.to_csv('../outs/trb_speed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7971e1-5f22-4789-9362-b0e909f7213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the duration in seconds per size\n",
    "fig, ax = plt.subplots(figsize=[6, 4]); ax.grid(False)\n",
    "data = df_time.loc[df_time['size'].isin(sizes[3:])]\n",
    "sns.barplot(x='size', y='duration', data=data, ci=95, errwidth=1.5, capsize=0.3, saturation=1,\n",
    "            errcolor='dodgerblue', edgecolor='dodgerblue', color='skyblue', linewidth=1.5)\n",
    "_ = ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "ax.set_yscale('log')\n",
    "ax.set(xlabel='Number of TCRs', ylabel='Inference Time (Seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1488c6c-0587-4102-a44d-fc4feeff7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the millions of TCRs inference times\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.plot(xs / 1e6, ys['mean'] / 60, color='dodgerblue', lw=2, zorder=2)\n",
    "ax.fill_between(xs / 1e6, (ys['mean'] - ys['mean_se']*1.96) / 60,\n",
    "               (ys['mean'] + ys['mean_se']*1.96) / 60, color='skyblue', lw=1.5, zorder=1, alpha=0.5)\n",
    "ax.set(xlabel='Millions of TCRs', ylabel='Inference Time (Minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2e01c-37b2-41ff-ae9e-e80670f9b5cf",
   "metadata": {},
   "source": [
    "### Compare with TCRdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e7ccc-a4cd-461b-aa80-08a2f1688125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwseqdist as pw\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "import pickle as pkl\n",
    "# load the pickled data\n",
    "with open('../external_data/results.tcr.pkl', 'rb') as f:\n",
    "    results_tcr = pkl.load(f)\n",
    "\n",
    "# read in the data\n",
    "df = pd.read_csv('../outs/df.int.clean.csv', index_col=0)\n",
    "a_trb = sc.read_h5ad('../outs/adata.trb.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a1ff5-d3af-48e7-9d14-36cf69af4b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tracking objects\n",
    "datas = []\n",
    "methods = ['TCRdist','Tarpon']\n",
    "epitopes = ['YLQPRTFLL','NLVPMVATV','TPRVTGGGAM','GILGFVFTL','GLCTLVAML','YVLDHLIVV','ELAGIGILTV','EAAGIGILTV',\n",
    "            'SLLMWITQC','KLGGALQAK','AVFDRKSDAK','RAKFKQLL','IVTDFSVIK','LLWNGPMAV','SPRWYFYYL','TTDPSFLGRY',\n",
    "            'RLRAEAQVK','LLLDRLNQL','LTDEMIAQY','CINGVCWTV','KTFPPTEPK','QYIKWPWYI','VMTTVLATL','DATYQRTRALVR',\n",
    "            'NQKLIANQF','FLCMKALLL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc57db1-55bc-44dd-8576-c8d1a4a6a4b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# loop through epitopes\n",
    "for epitope in tqdm(epitopes):\n",
    "    # derive the binding TCRs\n",
    "    np.random.seed(0)\n",
    "    # grab the binding TCRs\n",
    "    trbs = df['TRB'][df['AG'] == epitope].value_counts()\n",
    "    if len(trbs) == 0:\n",
    "        print(epitope)\n",
    "        continue\n",
    "    trbs = trbs.loc[trbs.index.isin(a_trb.obs.index)]\n",
    "    # trbs = trbs.loc[np.random.choice(trbs, size=min(100, len(trbs)), replace=False)]\n",
    "    # grab the irrelevant non-binding TCRs\n",
    "    trbs_rand = df['TRB'][df['AG'] != epitope].value_counts()\n",
    "    trbs_rand = trbs_rand.loc[trbs_rand.index.isin(a_trb.obs.index)]\n",
    "    trbs_rand = trbs_rand.loc[~trbs_rand.index.isin(trbs.index)]\n",
    "    trbs_rand = trbs_rand.loc[np.random.choice(trbs_rand.index, size=trbs.shape[0], replace=False)]\n",
    "    # confirm and aggregate\n",
    "    assert trbs.index.unique().shape[0] == trbs.index.shape[0]\n",
    "    assert trbs_rand.index.unique().shape[0] == trbs_rand.index.shape[0]\n",
    "    assert trbs.index.union(trbs_rand.index).shape[0] == (trbs.shape[0]+trbs_rand.shape[0])\n",
    "    trbs_agg = trbs.index.union(trbs_rand.index)\n",
    "    \n",
    "    # compute the pairwise distance via tcrdist-like metrics and tarpon\n",
    "    m2d = {}\n",
    "    m2d['TCRdist'] = pw.apply_pairwise_rect(seqs1 = trbs_agg.tolist(),\n",
    "                                            metric = pw.metrics.nb_vector_tcrdist, \n",
    "                                            ncpus = 5, use_numba = True, uniqify = True)\n",
    "    m2d['Tarpon'] = squareform(pdist(a_trb[trbs_agg].X))\n",
    "    \n",
    "    # loop through methods\n",
    "    for method in methods:\n",
    "        # copy over data for usage\n",
    "        dmet = m2d[method].copy().astype(float)\n",
    "        for idx in range(dmet.shape[0]):\n",
    "            dmet[idx, idx] = np.nan\n",
    "\n",
    "        # extract each pairwise comparison between groups\n",
    "        ag2rand = dmet[:dmet.shape[0]//2, dmet.shape[0]//2:].flatten()\n",
    "        ag2rand = ag2rand[~np.isnan(ag2rand)]\n",
    "        ag2ag = dmet[:dmet.shape[0]//2, :dmet.shape[0]//2].flatten()\n",
    "        ag2ag = ag2ag[~np.isnan(ag2ag)]\n",
    "        rand2rand = dmet[dmet.shape[0]//2:, dmet.shape[0]//2:].flatten()\n",
    "        rand2rand = rand2rand[~np.isnan(rand2rand)]\n",
    "        # convert to dataframes\n",
    "        ag2rand = pd.DataFrame(ag2rand, columns=['dist']); ag2rand['comp'] = 'ag2rand'\n",
    "        # aggregate and normalize\n",
    "        data = pd.concat([ag2rand], axis=0)\n",
    "        data['dist'] -= data['dist'].mean()\n",
    "        data['dist'] /= data['dist'].std()\n",
    "        # derive metrics\n",
    "        data['epitope'] = epitope\n",
    "        data['method'] = method\n",
    "        datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a6843-57ee-4f7e-85ca-05c4b63ce0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# derive averages for distance to antigen vs. random\n",
    "from tqdm import tqdm\n",
    "df_stat = pd.DataFrame(columns=['mean_a2r','epitope','method'])\n",
    "for data in tqdm(datas):\n",
    "    # retrieve relevant metrics\n",
    "    epitope = data['epitope'].iloc[0]\n",
    "    method = data['method'].iloc[0]\n",
    "    mean_a2r = data.loc[data['comp'] == 'ag2rand', 'dist'].mean()\n",
    "    # insert into dataframe\n",
    "    df_stat.loc[df_stat.shape[0]] = mean_a2r, epitope, method\n",
    "# save the data\n",
    "df_stat.to_csv('../outs/250425_compare_with_diffalgos.csv')\n",
    "with open('../outs/250425_compare_with_diffalgos.pkl', 'wb') as f:\n",
    "    pkl.dump(datas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd423a-ad51-403a-87b2-4714205f1f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compare the two relevant populations\n",
    "df_stat['enrich'] = df_stat['mean_a2r'].copy()\n",
    "fig, ax = plt.subplots(figsize=[1.5, 4]); ax.grid(False)\n",
    "sns.barplot(x='method', y='enrich', data=df_stat, ax=ax,\n",
    "            saturation=1, errcolor='dodgerblue', errwidth=1.5, order=['TCRdist','Tarpon'],\n",
    "            linewidth=1.5, capsize=0.3, edgecolor='dodgerblue', color='skyblue')\n",
    "np.random.seed(0)\n",
    "sns.stripplot(x='method', y='enrich', data=df_stat, ax=ax, alpha=0.5, order=['TCRdist','Tarpon'],\n",
    "              linewidth=1.5, edgecolor='dodgerblue', color='skyblue', jitter=0.35)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "ax.set_ylabel('Normalized Distance Between\\nAg-specific and Random TCRs')\n",
    "ss.wilcoxon(df_stat.loc[df_stat['method'] == 'Tarpon', 'enrich'],\n",
    "            df_stat.loc[df_stat['method'] == 'TCRdist', 'enrich'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu39",
   "language": "python",
   "name": "gpu39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
