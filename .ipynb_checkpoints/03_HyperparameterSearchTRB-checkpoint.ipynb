{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec786bc-44cc-4246-bdfa-8b30457ff5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "sc.settings.set_figure_params(dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160146b-b525-409a-8cd3-6f625bd67cb4",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6161fbf-0f77-411f-9376-37a142f6819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../external_data/db.ags.pkl', 'rb') as f: ags = pkl.load(f)\n",
    "with open('../external_data/db.tras.pkl', 'rb') as f: tras = pkl.load(f)\n",
    "with open('../external_data/db.trbs.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "with open('../external_data/db.paired_tcrs.pkl', 'rb') as f: paired_tcrs = pkl.load(f)\n",
    "ags, tras, trbs = pd.Series(ags), pd.Series(tras), pd.Series(trbs)\n",
    "print(len(ags)); print(len(tras)); print(len(trbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8248cb8-cb39-453c-ba3e-9c1706899e82",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f468d2-fe9f-4ba6-8513-3a92534906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5563f-e0bc-44d2-858c-29ac6be53240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (recon_batch, recon_len), mu, logvar = model(data)\n",
    "        if loss_func == 1:\n",
    "            loss = loss_function1(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 2:\n",
    "            loss = loss_function2(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 3:\n",
    "            loss = loss_function3(recon_batch, recon_len, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "    \n",
    "def test(epoch, loss_func):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data[0].to(device)\n",
    "            (recon_batch, recon_len), mu, logvar = model(data)\n",
    "            if loss_func == 1:\n",
    "                test_loss += loss_function1(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 2:\n",
    "                test_loss += loss_function2(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 3:\n",
    "                test_loss += loss_function3(recon_batch, recon_len, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093d493-1b5e-41ef-aee2-faf1352fdf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the key parameters\n",
    "init_embed_size = 50-1\n",
    "protein_len = 48\n",
    "init_kernel_size = 3\n",
    "init_cnn_filters = 128\n",
    "init_kernel_stride = 1\n",
    "init_kernel_padding = 1\n",
    "secn_cnn_filters = 128\n",
    "latent_dim = 32\n",
    "vocab = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "# we want the embedding output to be the vocab with the length to allow for reconstruction\n",
    "out_embed_size = len(vocab)\n",
    "n_nodes_len = 32\n",
    "\n",
    "# define the convolutional variational autoencoder\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # encoding\n",
    "        self.fc1 = nn.Conv1d(\n",
    "            in_channels=init_embed_size, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding,\n",
    "        )\n",
    "        self.fc2 = nn.Conv1d(\n",
    "            in_channels=init_cnn_filters, out_channels=secn_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        # variational sampling\n",
    "        self.fc31 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc32 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, secn_cnn_filters*protein_len)\n",
    "        # decoding\n",
    "        self.fc5 = nn.ConvTranspose1d(\n",
    "            in_channels=secn_cnn_filters, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc6 = nn.ConvTranspose1d(\n",
    "            in_channels=init_cnn_filters, out_channels=out_embed_size, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc7 = nn.Linear(init_cnn_filters*protein_len, n_nodes_len)\n",
    "        self.fc8 = nn.Linear(n_nodes_len, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x1 = nn.LeakyReLU()(self.fc1(x[:, :-1, :]))\n",
    "        x2 = nn.LeakyReLU()(self.fc2(x1))\n",
    "        x2_ = nn.Flatten()(x2)\n",
    "        return self.fc31(x2_), self.fc32(x2_)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x4 = nn.LeakyReLU()(self.fc4(z))\n",
    "        x4_ = x4.view(-1, secn_cnn_filters, protein_len)\n",
    "        x5 = nn.LeakyReLU()(self.fc5(x4_))\n",
    "        x5_ = nn.Flatten()(x5)\n",
    "        x6 = nn.Sigmoid()(self.fc6(x5))\n",
    "        return x6, self.fc8(nn.LeakyReLU()(self.fc7(x5_)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f560b5e-ac5d-4850-85e8-ea32b1e854f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function1(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "def loss_function2(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return TSE\n",
    "def loss_function3(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return BCE + KLD + TSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73ba37-ad8c-4af9-aa2b-ba09e4a52ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f537307-0f06-4e54-9e82-dc8d95a63db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the TRBs\n",
    "batch_size = 2048\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c112fd1-a3ac-4116-adce-10032109a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from Levenshtein import distance as levenshtein\n",
    "# define a method to reconstruct the sequence\n",
    "def reconstruct(tmp_out, targ_len, curr_len):\n",
    "    # compute the x-coordinates of the original\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, tmp_out[idx, :]) for idx in range(tmp_out.shape[0])])\n",
    "    return ''.join(pd.DataFrame(res, index=vocab).idxmax(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097418b-08c1-4e8a-ad3f-0021a7a1a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine functions to be silent\n",
    "def train(model, train_loader, epoch, loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (recon_batch, recon_len), mu, logvar = model(data)\n",
    "        if loss_func == 1:\n",
    "            loss = loss_function1(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 2:\n",
    "            loss = loss_function2(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 3:\n",
    "            loss = loss_function3(recon_batch, recon_len, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7a5cc-aaea-4575-844b-93b6f225a70e",
   "metadata": {},
   "source": [
    "### Search for Stable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bdc77f-7811-457e-b75c-b3e692c8b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants throughout the search\n",
    "device = 'cuda'\n",
    "batch_size = 2048\n",
    "# define a function to perform a single model search\n",
    "df_recons = []\n",
    "for seed in range(5):\n",
    "    print('.', end='')\n",
    "    # randomly split the data into train and test, select from a pool 100x batch size for speed\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    idxs_pool = np.random.choice(range(len(X_trbs)), size=round(100*batch_size), replace=False)\n",
    "    idxs_train = np.random.choice(idxs_pool, size=round(len(idxs_pool)*0.75), replace=False)\n",
    "    idxs_test = idxs_pool[~np.isin(idxs_pool, idxs_train)]\n",
    "    X_trbs_train = X_trbs[idxs_train]\n",
    "    X_trbs_test = X_trbs[idxs_test]\n",
    "    # create a latent space for the TRBs\n",
    "    train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # work through all model parameters\n",
    "    secn_cnn_filters, latent_dim, n_nodes_len = 256, 32, 32\n",
    "    for init_cnn_filters in [64, 128, 256, 512, 1024]:\n",
    "        # initialize the model\n",
    "        model = ConvVAE().to(device)\n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.0005; epochs = 20\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with dual losses to balance between two objectives\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=1)\n",
    "            train(model, train_loader, epoch, loss_func=2)\n",
    "        \n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.001; epochs = 40\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with an integrated loss\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=3)\n",
    "            \n",
    "        # retrieve the predictions\n",
    "        model.eval()\n",
    "        recon_lens = []; recon_batchs = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                data = data[0].to(device)\n",
    "                (recon_batch, recon_len), _, _ = model(data)\n",
    "                recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "                recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "        recon_lens = [x[0] for x in recon_lens]\n",
    "        # retrieve the indices\n",
    "        trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "        \n",
    "        # set parameters for reconstruction\n",
    "        curr_len = 48  # this is a constant\n",
    "        true_lens = trbs_test.apply(len)\n",
    "        n_sequences = len(trbs_test)\n",
    "        # test reconstruction keeping track in a dataframe\n",
    "        df_recon = pd.DataFrame(columns=['pred_len','true_len','true_seq','pred_seq_from_pred_len','pred_seq_from_true_len'])\n",
    "        for idx in range(n_sequences):\n",
    "            pred_len = recon_lens[idx]\n",
    "            true_len = true_lens.iloc[idx]\n",
    "            true_seq = trbs_test.iloc[idx]\n",
    "            recon_seq_from_pred_len = reconstruct(recon_batchs[idx], pred_len, curr_len)\n",
    "            recon_seq_from_true_len = reconstruct(recon_batchs[idx], true_len, curr_len)\n",
    "            df_recon.loc[idx] = pred_len, true_len, true_seq, recon_seq_from_pred_len, recon_seq_from_true_len\n",
    "        # assess via multiple metrics\n",
    "        df_recon['pred_len_diff'] = df_recon['pred_len'] - df_recon['true_len']\n",
    "        df_recon['leven_to_pred_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_pred_len']) for idx in df_recon.index]\n",
    "        df_recon['leven_to_true_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_true_len']) for idx in df_recon.index]\n",
    "        df_recon['init_cnn_filters'] = init_cnn_filters\n",
    "        df_recon['secn_cnn_filters'] = secn_cnn_filters\n",
    "        df_recon['latent_dim'] = latent_dim\n",
    "        df_recon['n_nodes_len'] = n_nodes_len\n",
    "        df_recon['seed'] = seed\n",
    "        df_recons.append(df_recon)\n",
    "# concatenate the data in order to examine the sequences\n",
    "df_recon_init = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36653889-eae0-4a9d-a3a2-f4fb178842d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to perform a single model search\n",
    "df_recons = []\n",
    "for seed in range(5):\n",
    "    print('.', end='')\n",
    "    # randomly split the data into train and test, select from a pool 100x batch size for speed\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    idxs_pool = np.random.choice(range(len(X_trbs)), size=round(100*batch_size), replace=False)\n",
    "    idxs_train = np.random.choice(idxs_pool, size=round(len(idxs_pool)*0.75), replace=False)\n",
    "    idxs_test = idxs_pool[~np.isin(idxs_pool, idxs_train)]\n",
    "    X_trbs_train = X_trbs[idxs_train]\n",
    "    X_trbs_test = X_trbs[idxs_test]\n",
    "    # create a latent space for the TRBs\n",
    "    train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # work through all model parameters\n",
    "    init_cnn_filters, latent_dim, n_nodes_len = 256, 32, 32\n",
    "    for secn_cnn_filters in [64, 128, 256, 512, 1024]:\n",
    "        # initialize the model\n",
    "        model = ConvVAE().to(device)\n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.0005; epochs = 20\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with dual losses to balance between two objectives\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=1)\n",
    "            train(model, train_loader, epoch, loss_func=2)\n",
    "        \n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.001; epochs = 40\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with an integrated loss\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=3)\n",
    "            \n",
    "        # retrieve the predictions\n",
    "        model.eval()\n",
    "        recon_lens = []; recon_batchs = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                data = data[0].to(device)\n",
    "                (recon_batch, recon_len), _, _ = model(data)\n",
    "                recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "                recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "        recon_lens = [x[0] for x in recon_lens]\n",
    "        # retrieve the indices\n",
    "        trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "        \n",
    "        # set parameters for reconstruction\n",
    "        curr_len = 48  # this is a constant\n",
    "        true_lens = trbs_test.apply(len)\n",
    "        n_sequences = len(trbs_test)\n",
    "        # test reconstruction keeping track in a dataframe\n",
    "        df_recon = pd.DataFrame(columns=['pred_len','true_len','true_seq','pred_seq_from_pred_len','pred_seq_from_true_len'])\n",
    "        for idx in range(n_sequences):\n",
    "            pred_len = recon_lens[idx]\n",
    "            true_len = true_lens.iloc[idx]\n",
    "            true_seq = trbs_test.iloc[idx]\n",
    "            recon_seq_from_pred_len = reconstruct(recon_batchs[idx], pred_len, curr_len)\n",
    "            recon_seq_from_true_len = reconstruct(recon_batchs[idx], true_len, curr_len)\n",
    "            df_recon.loc[idx] = pred_len, true_len, true_seq, recon_seq_from_pred_len, recon_seq_from_true_len\n",
    "        # assess via multiple metrics\n",
    "        df_recon['pred_len_diff'] = df_recon['pred_len'] - df_recon['true_len']\n",
    "        df_recon['leven_to_pred_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_pred_len']) for idx in df_recon.index]\n",
    "        df_recon['leven_to_true_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_true_len']) for idx in df_recon.index]\n",
    "        df_recon['init_cnn_filters'] = init_cnn_filters\n",
    "        df_recon['secn_cnn_filters'] = secn_cnn_filters\n",
    "        df_recon['latent_dim'] = latent_dim\n",
    "        df_recon['n_nodes_len'] = n_nodes_len\n",
    "        df_recon['seed'] = seed\n",
    "        df_recons.append(df_recon)\n",
    "# concatenate the data in order to examine the sequences\n",
    "df_recon_secn = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac8a5f-bc7f-4638-af82-cbcfc1469eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to perform a single model search\n",
    "df_recons = []\n",
    "for seed in range(5):\n",
    "    print('.', end='')\n",
    "    # randomly split the data into train and test, select from a pool 100x batch size for speed\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    idxs_pool = np.random.choice(range(len(X_trbs)), size=round(100*batch_size), replace=False)\n",
    "    idxs_train = np.random.choice(idxs_pool, size=round(len(idxs_pool)*0.75), replace=False)\n",
    "    idxs_test = idxs_pool[~np.isin(idxs_pool, idxs_train)]\n",
    "    X_trbs_train = X_trbs[idxs_train]\n",
    "    X_trbs_test = X_trbs[idxs_test]\n",
    "    # create a latent space for the TRBs\n",
    "    train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # work through all model parameters\n",
    "    init_cnn_filters, secn_cnn_filters, n_nodes_len = 256, 256, 32\n",
    "    for latent_dim in [8, 16, 32, 64, 128]:\n",
    "        # initialize the model\n",
    "        model = ConvVAE().to(device)\n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.0005; epochs = 20\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with dual losses to balance between two objectives\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=1)\n",
    "            train(model, train_loader, epoch, loss_func=2)\n",
    "        \n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.001; epochs = 40\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with an integrated loss\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=3)\n",
    "            \n",
    "        # retrieve the predictions\n",
    "        model.eval()\n",
    "        recon_lens = []; recon_batchs = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                data = data[0].to(device)\n",
    "                (recon_batch, recon_len), _, _ = model(data)\n",
    "                recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "                recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "        recon_lens = [x[0] for x in recon_lens]\n",
    "        # retrieve the indices\n",
    "        trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "        \n",
    "        # set parameters for reconstruction\n",
    "        curr_len = 48  # this is a constant\n",
    "        true_lens = trbs_test.apply(len)\n",
    "        n_sequences = len(trbs_test)\n",
    "        # test reconstruction keeping track in a dataframe\n",
    "        df_recon = pd.DataFrame(columns=['pred_len','true_len','true_seq','pred_seq_from_pred_len','pred_seq_from_true_len'])\n",
    "        for idx in range(n_sequences):\n",
    "            pred_len = recon_lens[idx]\n",
    "            true_len = true_lens.iloc[idx]\n",
    "            true_seq = trbs_test.iloc[idx]\n",
    "            recon_seq_from_pred_len = reconstruct(recon_batchs[idx], pred_len, curr_len)\n",
    "            recon_seq_from_true_len = reconstruct(recon_batchs[idx], true_len, curr_len)\n",
    "            df_recon.loc[idx] = pred_len, true_len, true_seq, recon_seq_from_pred_len, recon_seq_from_true_len\n",
    "        # assess via multiple metrics\n",
    "        df_recon['pred_len_diff'] = df_recon['pred_len'] - df_recon['true_len']\n",
    "        df_recon['leven_to_pred_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_pred_len']) for idx in df_recon.index]\n",
    "        df_recon['leven_to_true_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_true_len']) for idx in df_recon.index]\n",
    "        df_recon['init_cnn_filters'] = init_cnn_filters\n",
    "        df_recon['secn_cnn_filters'] = secn_cnn_filters\n",
    "        df_recon['latent_dim'] = latent_dim\n",
    "        df_recon['n_nodes_len'] = n_nodes_len\n",
    "        df_recon['seed'] = seed\n",
    "        df_recons.append(df_recon)\n",
    "# concatenate the data in order to examine the sequences\n",
    "df_recon_ldim = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36159bf-ccfd-42bc-9c1c-5c5a95c484c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to perform a single model search\n",
    "df_recons = []\n",
    "for seed in range(5):\n",
    "    print('.', end='')\n",
    "    # randomly split the data into train and test, select from a pool 100x batch size for speed\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    idxs_pool = np.random.choice(range(len(X_trbs)), size=round(100*batch_size), replace=False)\n",
    "    idxs_train = np.random.choice(idxs_pool, size=round(len(idxs_pool)*0.75), replace=False)\n",
    "    idxs_test = idxs_pool[~np.isin(idxs_pool, idxs_train)]\n",
    "    X_trbs_train = X_trbs[idxs_train]\n",
    "    X_trbs_test = X_trbs[idxs_test]\n",
    "    # create a latent space for the TRBs\n",
    "    train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # work through all model parameters\n",
    "    init_cnn_filters, secn_cnn_filters, latent_dim = 256, 256, 32\n",
    "    for n_nodes_len in [8, 16, 32, 64, 128]:\n",
    "        # initialize the model\n",
    "        model = ConvVAE().to(device)\n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.0005; epochs = 20\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with dual losses to balance between two objectives\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=1)\n",
    "            train(model, train_loader, epoch, loss_func=2)\n",
    "        \n",
    "        # set the seed for training\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "        # set the learning parameters\n",
    "        lr = 0.001; epochs = 40\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        # train the model with an integrated loss\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train(model, train_loader, epoch, loss_func=3)\n",
    "            \n",
    "        # retrieve the predictions\n",
    "        model.eval()\n",
    "        recon_lens = []; recon_batchs = []\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                data = data[0].to(device)\n",
    "                (recon_batch, recon_len), _, _ = model(data)\n",
    "                recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "                recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "        recon_lens = [x[0] for x in recon_lens]\n",
    "        # retrieve the indices\n",
    "        trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "        \n",
    "        # set parameters for reconstruction\n",
    "        curr_len = 48  # this is a constant\n",
    "        true_lens = trbs_test.apply(len)\n",
    "        n_sequences = len(trbs_test)\n",
    "        # test reconstruction keeping track in a dataframe\n",
    "        df_recon = pd.DataFrame(columns=['pred_len','true_len','true_seq','pred_seq_from_pred_len','pred_seq_from_true_len'])\n",
    "        for idx in range(n_sequences):\n",
    "            pred_len = recon_lens[idx]\n",
    "            true_len = true_lens.iloc[idx]\n",
    "            true_seq = trbs_test.iloc[idx]\n",
    "            recon_seq_from_pred_len = reconstruct(recon_batchs[idx], pred_len, curr_len)\n",
    "            recon_seq_from_true_len = reconstruct(recon_batchs[idx], true_len, curr_len)\n",
    "            df_recon.loc[idx] = pred_len, true_len, true_seq, recon_seq_from_pred_len, recon_seq_from_true_len\n",
    "        # assess via multiple metrics\n",
    "        df_recon['pred_len_diff'] = df_recon['pred_len'] - df_recon['true_len']\n",
    "        df_recon['leven_to_pred_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_pred_len']) for idx in df_recon.index]\n",
    "        df_recon['leven_to_true_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_true_len']) for idx in df_recon.index]\n",
    "        df_recon['init_cnn_filters'] = init_cnn_filters\n",
    "        df_recon['secn_cnn_filters'] = secn_cnn_filters\n",
    "        df_recon['latent_dim'] = latent_dim\n",
    "        df_recon['n_nodes_len'] = n_nodes_len\n",
    "        df_recon['seed'] = seed\n",
    "        df_recons.append(df_recon)\n",
    "# concatenate the data in order to examine the sequences\n",
    "df_recon_nlen = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347f3e8-452b-4e49-9b0c-d496f1956895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each search to the disk\n",
    "df_recon_init.to_csv('../outs/matrix.gridsearch_init.final.trb.csv')\n",
    "df_recon_secn.to_csv('../outs/matrix.gridsearch_secn.final.trb.csv')\n",
    "df_recon_ldim.to_csv('../outs/matrix.gridsearch_ldim.final.trb.csv')\n",
    "df_recon_nlen.to_csv('../outs/matrix.gridsearch_nlen.final.trb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba458d4f-7ffa-4712-91f5-cbbcf9d6d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine different latent dimensions\n",
    "data = df_recon_init.groupby(['init_cnn_filters','seed']).mean(numeric_only=True).reset_index()\n",
    "data['pred_len_diff_abs'] = data['pred_len_diff'].abs()\n",
    "for col in ['pred_len_diff_abs','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    np.random.seed(0)\n",
    "    fig, ax = plt.subplots(figsize=[3.5, 4]); ax.grid(False)\n",
    "    sns.boxplot(x='init_cnn_filters', y=col, data=data, saturation=1, linecolor='dodgerblue', color='skyblue', linewidth=1.5, showfliers=False)\n",
    "    sns.stripplot(x='init_cnn_filters', y=col, data=data, edgecolor='dodgerblue', color='skyblue', linewidth=1.5, jitter=0.25, alpha=0.5)\n",
    "    ax.set_xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea4c85-c929-47eb-89bf-52f47bbc8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve p-values\n",
    "for col in ['pred_len_diff','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    res = ss.kruskal(*[data.loc[data['init_cnn_filters'] == x, col].abs().tolist() for x in data['init_cnn_filters'].unique()])\n",
    "    print(col, res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3fb26-592e-4c8f-a607-45794649e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine different latent dimensions\n",
    "data = df_recon_secn.groupby(['secn_cnn_filters','seed']).mean(numeric_only=True).reset_index()\n",
    "data['pred_len_diff_abs'] = data['pred_len_diff'].abs()\n",
    "for col in ['pred_len_diff_abs','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    np.random.seed(0)\n",
    "    fig, ax = plt.subplots(figsize=[3.5, 4]); ax.grid(False)\n",
    "    sns.boxplot(x='secn_cnn_filters', y=col, data=data, saturation=1, linecolor='dodgerblue', color='skyblue', linewidth=1.5, showfliers=False)\n",
    "    sns.stripplot(x='secn_cnn_filters', y=col, data=data, edgecolor='dodgerblue', color='skyblue', linewidth=1.5, jitter=0.25, alpha=0.5)\n",
    "    ax.set_xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63069ad6-a9f1-430d-8c96-dfab0a4eca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve p-values\n",
    "for col in ['pred_len_diff','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    res = ss.kruskal(*[data.loc[data['secn_cnn_filters'] == x, col].abs().tolist() for x in data['secn_cnn_filters'].unique()])\n",
    "    print(col, res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ac591-c5e4-4a17-b3c5-f279d82ac864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine different latent dimensions\n",
    "data = df_recon_ldim.groupby(['latent_dim','seed']).mean(numeric_only=True).reset_index()\n",
    "data['pred_len_diff_abs'] = data['pred_len_diff'].abs()\n",
    "for col in ['pred_len_diff_abs','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    np.random.seed(0)\n",
    "    fig, ax = plt.subplots(figsize=[3.5, 4]); ax.grid(False)\n",
    "    sns.boxplot(x='latent_dim', y=col, data=data, saturation=1, linecolor='dodgerblue', color='skyblue', linewidth=1.5, showfliers=False)\n",
    "    sns.stripplot(x='latent_dim', y=col, data=data, edgecolor='dodgerblue', color='skyblue', linewidth=1.5, jitter=0.25, alpha=0.5)\n",
    "    ax.set_xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3806e-efbd-4d3d-abd2-42daee03cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve p-values\n",
    "for col in ['pred_len_diff','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    res = ss.kruskal(*[data.loc[data['latent_dim'] == x, col].abs().tolist() for x in data['latent_dim'].unique()])\n",
    "    print(col, res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdf922-8710-4065-a50e-8abbd25ab7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine different latent dimensions\n",
    "data = df_recon_nlen.groupby(['n_nodes_len','seed']).mean(numeric_only=True).reset_index()\n",
    "data['pred_len_diff_abs'] = data['pred_len_diff'].abs()\n",
    "for col in ['pred_len_diff_abs','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    np.random.seed(0)\n",
    "    fig, ax = plt.subplots(figsize=[3.5, 4]); ax.grid(False)\n",
    "    sns.boxplot(x='n_nodes_len', y=col, data=data, saturation=1, linecolor='dodgerblue', color='skyblue', linewidth=1.5, showfliers=False)\n",
    "    sns.stripplot(x='n_nodes_len', y=col, data=data, edgecolor='dodgerblue', color='skyblue', linewidth=1.5, jitter=0.25, alpha=0.5)\n",
    "    ax.set_xlim(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a0d87-db57-48b0-baf9-8a528f0f3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve p-values\n",
    "for col in ['pred_len_diff','leven_to_pred_len_recon','leven_to_true_len_recon']:\n",
    "    res = ss.kruskal(*[data.loc[data['n_nodes_len'] == x, col].abs().tolist() for x in data['n_nodes_len'].unique()])\n",
    "    print(col, res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2680c8-1334-4032-9951-39735dad2565",
   "metadata": {},
   "source": [
    "### Examine Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154fefb-21ac-4a4a-9290-d9f44f3150ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (recon_batch, recon_len), mu, logvar = model(data)\n",
    "        if loss_func == 1:\n",
    "            loss = loss_function1(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 2:\n",
    "            loss = loss_function2(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 3:\n",
    "            loss = loss_function3(recon_batch, recon_len, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "    \n",
    "def test(epoch, loss_func):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data[0].to(device)\n",
    "            (recon_batch, recon_len), mu, logvar = model(data)\n",
    "            if loss_func == 1:\n",
    "                test_loss += loss_function1(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 2:\n",
    "                test_loss += loss_function2(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 3:\n",
    "                test_loss += loss_function3(recon_batch, recon_len, data, mu, logvar).item()\n",
    "    return test_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2955d-dbb1-4f7b-906b-fc55e4963057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# define the best hyperparameters / lowest we can get away with\n",
    "init_cnn_filters = 256\n",
    "secn_cnn_filters = 256\n",
    "latent_dim = 32\n",
    "n_nodes_len = 32\n",
    "# define standard testing parameters\n",
    "batch_size = 2048\n",
    "device = 'cuda'\n",
    "train_losses_12, test_losses_12, train_losses_3, test_losses_3, df_recons = [], [], [], [], []\n",
    "for seed in range(4, 5):\n",
    "    # randomly split the data into train and test\n",
    "    print('splitting the data...', end='')\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "    idxs_test = np.array(range(len(X_trbs)))\n",
    "    idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "    X_trbs_train = X_trbs[idxs_train]\n",
    "    X_trbs_test = X_trbs[idxs_test]\n",
    "    # create dataloaders\n",
    "    train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "    # initialize the model\n",
    "    print('creating the model...', end='')\n",
    "    model = ConvVAE().to(device)\n",
    "    # set the seed for training\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    # set the learning parameters\n",
    "    lr = 0.0005; epochs = 20\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # train the model with dual losses to balance between two objectives\n",
    "    train_losses = []; test_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses_, test_losses_ = [], []\n",
    "        for loss_func in [1, 2]:\n",
    "            train_losses_.append(train(epoch, loss_func))\n",
    "            test_losses_.append(test(epoch, loss_func))\n",
    "        train_losses.append(train_losses_)\n",
    "        test_losses.append(test_losses_)\n",
    "    train_losses_12.append(train_losses)\n",
    "    test_losses_12.append(test_losses)\n",
    "\n",
    "    # set the seed for training\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    # set the learning parameters\n",
    "    lr = 0.001; epochs = 40\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # train the model with an integrated loss\n",
    "    train_losses = []; test_losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses.append(train(epoch, loss_func=3))\n",
    "        test_losses.append(test(epoch, loss_func=3))\n",
    "    train_losses_3.append(train_losses)\n",
    "    test_losses_3.append(test_losses)\n",
    "    \n",
    "    # compute additional metrics via predictions\n",
    "    model.eval()\n",
    "    print('testing...', end='')\n",
    "    recon_lens = []; recon_batchs = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data[0].to(device)\n",
    "            (recon_batch, recon_len), _, _ = model(data)\n",
    "            recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "            recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "    recon_lens = [x[0] for x in recon_lens]\n",
    "    # retrieve the indices\n",
    "    trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "    \n",
    "    # set parameters for reconstruction\n",
    "    print('scoring the data...', end='')\n",
    "    curr_len = 48  # this is a constant\n",
    "    true_lens = trbs_test.apply(len)\n",
    "    n_sequences = len(trbs_test)\n",
    "    values = np.random.choice(range(n_sequences), size=1000, replace=False)\n",
    "    # test reconstruction keeping track in a dataframe\n",
    "    df_recon = pd.DataFrame(columns=['pred_len','true_len','true_seq','pred_seq_from_pred_len','pred_seq_from_true_len'])\n",
    "    for idx in values:\n",
    "        pred_len = recon_lens[idx]\n",
    "        true_len = true_lens.iloc[idx]\n",
    "        true_seq = trbs_test.iloc[idx]\n",
    "        recon_seq_from_pred_len = reconstruct(recon_batchs[idx], pred_len, curr_len)\n",
    "        recon_seq_from_true_len = reconstruct(recon_batchs[idx], true_len, curr_len)\n",
    "        df_recon.loc[idx] = pred_len, true_len, true_seq, recon_seq_from_pred_len, recon_seq_from_true_len\n",
    "    # assess via multiple metrics\n",
    "    print('saving this round...')\n",
    "    df_recon['pred_len_diff'] = df_recon['pred_len'] - df_recon['true_len']\n",
    "    df_recon['leven_to_pred_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_pred_len']) for idx in df_recon.index]\n",
    "    df_recon['leven_to_true_len_recon'] = [levenshtein(df_recon.loc[idx, 'true_seq'], df_recon.loc[idx, 'pred_seq_from_true_len']) for idx in df_recon.index]\n",
    "    df_recon['init_cnn_filters'] = init_cnn_filters\n",
    "    df_recon['secn_cnn_filters'] = secn_cnn_filters\n",
    "    df_recon['latent_dim'] = latent_dim\n",
    "    df_recon['n_nodes_len'] = n_nodes_len\n",
    "    df_recon['seed'] = seed\n",
    "    df_recons.append(df_recon)\n",
    "df_recon = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]\n",
    "df_recon['pred_len_diff_abs'] = df_recon['pred_len_diff'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055a762-f92b-463b-8ba1-0fe8da46c66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump the results\n",
    "result = (train_losses_12, test_losses_12, train_losses_3, test_losses_3, df_recons)\n",
    "import pickle as pkl\n",
    "with open('../models/trb.fiveiter.results.pkl', 'wb') as f:\n",
    "    pkl.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccfeb2-582a-4ac6-bce6-a0f5642fdaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump the results\n",
    "import pickle as pkl\n",
    "with open('../models/trb.fiveiter.results.pkl', 'rb') as f:\n",
    "    train_losses_12, test_losses_12, train_losses_3, test_losses_3, df_recons = pkl.load(f)\n",
    "df_recon = pd.concat(df_recons, axis=0).reset_index().iloc[:, 1:]\n",
    "df_recon['pred_len_diff_abs'] = df_recon['pred_len_diff'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af1690-c7d4-40fa-9099-e94ae3beda95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the difference\n",
    "ys = ['pred_len_diff','pred_len_diff_abs','leven_to_pred_len_recon','leven_to_true_len_recon']\n",
    "for y in ys:\n",
    "    fig, ax = plt.subplots(figsize=[1.5, 4]); ax.grid(False)\n",
    "    sns.boxplot(y=y, data=df_recon.groupby('seed').mean(numeric_only=True),\n",
    "                linewidth=1.5, saturation=1, showfliers=False, linecolor='dodgerblue', color='skyblue')\n",
    "    sns.stripplot(y=y, data=df_recon.groupby('seed').mean(numeric_only=True),\n",
    "                  linewidth=1.5, s=6, alpha=0.5, color='skyblue', edgecolor='dodgerblue')\n",
    "    ax.set_xlim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508e60b-736b-413b-9beb-b24941d64118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar plot with colors as percentage as y-axis as counts\n",
    "def plot_bar(counts, cmap, edgecolor, labelrotation=90, figsize=None, color=None):\n",
    "    # convert to relevant colors\n",
    "    if color is None:\n",
    "        colors = counts / counts.sum()\n",
    "        colors = [to_hex(cmap(x)) for x in colors]\n",
    "    else:\n",
    "        colors = [color]*len(counts)\n",
    "    figsize = [6, 4] if figsize is None else figsize\n",
    "    fig, ax = plt.subplots(figsize=figsize); ax.grid(False)\n",
    "    ax.bar(counts.index, counts, edgecolor=edgecolor, lw=1.5, color=colors)\n",
    "    ax.tick_params(axis='x', labelrotation=labelrotation)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb0644-3ea9-49b5-b44a-b05854bd60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.colors import to_hex\n",
    "# provide examples at each section\n",
    "cmap = get_cmap('Blues')\n",
    "fig, ax = plt.subplots(figsize=[6, 4]); ax.grid(False)\n",
    "# take the average counts\n",
    "counts = []\n",
    "for seed in range(5):\n",
    "    count = df_recon.loc[df_recon['seed'] == seed, 'leven_to_pred_len_recon'].value_counts()\n",
    "    order = sorted(count.index); count = count.loc[order]; counts.append(count)\n",
    "count = pd.concat(counts, axis=0).reset_index(); count.columns = ['x','y']\n",
    "order = list(range(min(count['x']), max(count['x'])+1))\n",
    "sns.barplot(x='x', y='y', data=count, order=order, edgecolor='dodgerblue', color='skyblue', linewidth=1.5,\n",
    "            saturation=1, err_kws={'color': 'dodgerblue', 'linewidth': 1.5}, capsize=0.3, errorbar=('ci', 95))\n",
    "ax.set(xlabel='Levenshtein distance prediction vs. truth', ylabel='N-Observations')\n",
    "ax.set_xlim(-1, order[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6422e1-6bf5-4680-81da-53492b59560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are perfectly predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 0]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e3be4-e947-4809-b53d-6f95158a43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are almost predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 1]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d391e-04b3-4895-a241-f98da50facc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are almost predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 2]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe96b49-0cce-445d-ad90-2a5ecfb20533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are almost predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 3]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343616fb-02c0-456e-b254-21926dd6d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are almost predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 4]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d855ef-8ff4-47c1-bdd1-1716e63770c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a few sequences that are almost predicted\n",
    "np.random.seed(0)\n",
    "idxs = df_recon.index[df_recon['leven_to_pred_len_recon'] == 5]\n",
    "idxs = np.random.choice(idxs, size=5)\n",
    "df_recon.loc[idxs, ['true_seq','pred_seq_from_pred_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678054e3-33e6-4ab7-98aa-7eb21fce6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide examples at each difference between predicted and true length\n",
    "fig, ax = plt.subplots(figsize=[6, 4]); ax.grid(False)\n",
    "for seed in range(5):\n",
    "    values = df_recon.loc[df_recon['seed'] == seed, 'pred_len_diff_abs']\n",
    "    sns.kdeplot(values, label=seed+1, color=to_hex(cmap((seed+2)/8)), lw=1.5, bw_method=0.2)\n",
    "ax.set(xlabel='Length difference', ylabel='N-Observations')\n",
    "ax.legend(bbox_to_anchor=(.99, .5), bbox_transform=ax.transAxes, loc='center left', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72817073-881a-4025-80e5-219eb95c6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide examples at each difference between predicted and true length\n",
    "fig, ax = plt.subplots(figsize=[6, 4]); ax.grid(False)\n",
    "for seed in range(5):\n",
    "    values = df_recon.loc[df_recon['seed'] == seed, 'pred_len_diff']\n",
    "    sns.kdeplot(values, label=seed+1, color=to_hex(cmap((seed+2)/8)), lw=1.5, bw_method=0.2)\n",
    "ax.set(xlabel='Levenshtein distance prediction vs. truth', ylabel='N-Observations')\n",
    "ax.legend(bbox_to_anchor=(.99, .5), bbox_transform=ax.transAxes, loc='center left', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c58a4-c96f-4e65-986e-7270e9b5b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display losses for training and testing\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "for idx, (losses_train, losses_test) in enumerate(zip(train_losses_12, test_losses_12)):\n",
    "    losses_train = np.array(losses_train); losses_test = np.array(losses_test)\n",
    "    ax.plot(losses_train[:, 0], label=idx+1, color=to_hex(cmap((idx+2)/8)), lw=1.5)\n",
    "    ax.plot(losses_test[:, 0], color=to_hex(cmap((idx+2)/8)), lw=1.5, linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='Loss #1')\n",
    "ax.legend(bbox_to_anchor=(.99, .5), bbox_transform=ax.transAxes, loc='center left', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723fb26-00f3-431c-a7f9-e5c951cba7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display losses for training and testing\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "for idx, (losses_train, losses_test) in enumerate(zip(train_losses_12, test_losses_12)):\n",
    "    losses_train = np.array(losses_train); losses_test = np.array(losses_test)\n",
    "    ax.plot(losses_train[:, 1], label=idx+1, color=to_hex(cmap((idx+2)/8)), lw=1.5)\n",
    "    ax.plot(losses_test[:, 1], color=to_hex(cmap((idx+2)/8)), lw=1.5, linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='Loss #2')\n",
    "ax.legend(bbox_to_anchor=(.99, .5), bbox_transform=ax.transAxes, loc='center left', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe27930-d6af-4092-920b-327ca31468fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display losses for training and testing\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "for idx, (losses_train, losses_test) in enumerate(zip(train_losses_3, test_losses_3)):\n",
    "    ax.plot(losses_train, label=idx+1, color=to_hex(cmap((idx+2)/8)), lw=1.5)\n",
    "    ax.plot(losses_test, color=to_hex(cmap((idx+2)/8)), lw=1.5, linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='Loss #3 (FineTune)')\n",
    "ax.legend(bbox_to_anchor=(.99, .5), bbox_transform=ax.transAxes, loc='center left', frameon=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu39",
   "language": "python",
   "name": "gpu39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
