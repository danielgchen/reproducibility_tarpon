{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a5e4a-85e6-4326-847c-0b768b755633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.colors import to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "sc.settings.set_figure_params(dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea729b-d747-4a08-9263-f58e40ce62ef",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ced73-749f-4bcb-b86f-50b140b930b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../external_data/db.ags.pkl', 'rb') as f: ags = pkl.load(f)\n",
    "with open('../external_data/db.tras.pkl', 'rb') as f: tras = pkl.load(f)\n",
    "with open('../external_data/db.trbs.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "with open('../external_data/db.paired_tcrs.pkl', 'rb') as f: paired_tcrs = pkl.load(f)\n",
    "ags, tras, trbs = pd.Series(ags), pd.Series(tras), pd.Series(trbs)\n",
    "\n",
    "# trim the sequences\n",
    "# cut all of the trbs by 1-4AA on each side\n",
    "trbs4 = [x[4:-4] for x in trbs if len(x[4:-4]) > 0]\n",
    "trbs3 = [x[3:-3] for x in trbs if len(x[3:-3]) > 0]\n",
    "trbs2 = [x[2:-2] for x in trbs if len(x[2:-2]) > 0]\n",
    "trbs1 = [x[1:-1] for x in trbs if len(x[1:-1]) > 0]\n",
    "len(trbs4), len(trbs3), len(trbs2), len(trbs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2c838-0eb8-44b5-a6fd-92de5ab19b74",
   "metadata": {},
   "source": [
    "### Perform Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ef593-301f-4b3e-9991-2ae20c392d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blosum as bl\n",
    "# perform encoding by direct, BCP, BLOSUM\n",
    "vocab = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "# direct encoding\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "# bcp encoding\n",
    "aa_hydrophobicity = {\n",
    "    'A': 1.8,  # Alanine\n",
    "    'R': -4.5,  # Arginine\n",
    "    'N': -3.5,  # Asparagine\n",
    "    'D': -3.5,  # Aspartic Acid\n",
    "    'C': 2.5,  # Cysteine\n",
    "    'E': -3.5,  # Glutamic Acid\n",
    "    'Q': -3.5,  # Glutamine\n",
    "    'G': -0.4,  # Glycine\n",
    "    'H': -3.2,  # Histidine\n",
    "    'I': 4.5,  # Isoleucine\n",
    "    'L': 3.8,  # Leucine\n",
    "    'K': -3.9,  # Lysine\n",
    "    'M': 1.9,  # Methionine\n",
    "    'F': 2.8,  # Phenylalanine\n",
    "    'P': -1.6,  # Proline\n",
    "    'S': -0.8,  # Serine\n",
    "    'T': -0.7,  # Threonine\n",
    "    'W': -0.9,  # Tryptophan\n",
    "    'Y': -1.3,  # Tyrosine\n",
    "    'V': 4.2,  # Valine\n",
    "}\n",
    "# https://www.imgt.org/IMGTeducation/Aide-memoire/_UK/aminoacids/IMGTclasses.html\n",
    "aa_volume = {\n",
    "    'A': 88.6,   # Alanine\n",
    "    'R': 173.4,  # Arginine\n",
    "    'N': 114.1,  # Asparagine\n",
    "    'D': 111.1,  # Aspartic Acid\n",
    "    'C': 108.5,  # Cysteine\n",
    "    'E': 138.4,  # Glutamic Acid\n",
    "    'Q': 143.8,  # Glutamine\n",
    "    'G': 60.1,   # Glycine\n",
    "    'H': 153.2,  # Histidine\n",
    "    'I': 166.7,  # Isoleucine\n",
    "    'L': 166.7,  # Leucine\n",
    "    'K': 168.6,  # Lysine\n",
    "    'M': 162.9,  # Methionine\n",
    "    'F': 189.9,  # Phenylalanine\n",
    "    'P': 112.7,  # Proline\n",
    "    'S': 89.0,   # Serine\n",
    "    'T': 116.1,  # Threonine\n",
    "    'W': 227.8,  # Tryptophan\n",
    "    'Y': 193.6,  # Tyrosine\n",
    "    'V': 140.0,  # Valine\n",
    "}\n",
    "# 1 = donor and acceptor, 0.5 = only donor or acceptor\n",
    "aa_hbond = {\n",
    "    'A': 0,    # Alanine\n",
    "    'R': 0.5,  # Arginine\n",
    "    'N': 1,    # Asparagine\n",
    "    'D': 0.5,  # Aspartic Acid\n",
    "    'C': 0,    # Cysteine\n",
    "    'E': 0.5,  # Glutamic Acid\n",
    "    'Q': 1,    # Glutamine\n",
    "    'G': 0,    # Glycine\n",
    "    'H': 1,    # Histidine\n",
    "    'I': 0,    # Isoleucine\n",
    "    'L': 0,    # Leucine\n",
    "    'K': 0.5,  # Lysine\n",
    "    'M': 0,    # Methionine\n",
    "    'F': 0,    # Phenylalanine\n",
    "    'P': 0,    # Proline\n",
    "    'S': 1,    # Serine\n",
    "    'T': 1,    # Threonine\n",
    "    'W': 0.5,  # Tryptophan\n",
    "    'Y': 1,    # Tyrosine\n",
    "    'V': 0,    # Valine\n",
    "}\n",
    "has_sulfur = ['C','M']\n",
    "is_aromatic = ['F','Y','W']\n",
    "is_aliphatic = ['A','G','I','L','P','V']\n",
    "is_basic = ['R','H','K']\n",
    "is_acidic = ['D','E']\n",
    "has_amide = ['N','Q']\n",
    "vocab_bcp = ['hydrophobicity','volume','hbond','has_sulfur','is_aromatic',\n",
    "             'is_aliphatic','is_basic','is_acidic','has_amide']\n",
    "# > normalize the data for both volume and charge\n",
    "vmin, vmax = min(list(aa_volume.values())), max(list(aa_volume.values()))\n",
    "aa_volume = {k:(v-vmin)/(vmax-vmin) for k,v in aa_volume.items()}\n",
    "vmax = max(abs(np.array(list(aa_hydrophobicity.values()))))\n",
    "aa_hydrophobicity = {k:v/vmax for k,v in aa_hydrophobicity.items()}\n",
    "# > define a method to return the embedding for a given amino acid in BCP space\n",
    "def bcp_translation(aa):\n",
    "    embedding = []\n",
    "    embedding.append(aa_hydrophobicity[aa])\n",
    "    embedding.append(aa_volume[aa])\n",
    "    embedding.append(aa_hbond[aa])\n",
    "    embedding.append(1 * (aa in has_sulfur))\n",
    "    embedding.append(1 * (aa in is_aromatic))\n",
    "    embedding.append(1 * (aa in is_aliphatic))\n",
    "    embedding.append(1 * (aa in is_basic))\n",
    "    embedding.append(1 * (aa in is_acidic))\n",
    "    embedding.append(1 * (aa in has_amide))\n",
    "    return embedding\n",
    "map_bcp = {x:bcp_translation(x) for x in vocab}\n",
    "map_blosum = {x:[bl.BLOSUM(62)[x][y] / 5 for y in vocab] for x in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9a141-e87e-41a6-87aa-11ad4422c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to embed an amino acid with direct, bcp, blosum, and length\n",
    "def embed_aa(aa):\n",
    "    embed = [x for x in map_direct[aa]]\n",
    "    embed += map_bcp[aa]\n",
    "    embed += map_blosum[aa]\n",
    "    embed += [0]\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7379ff2-a64f-4728-8198-488f95c61de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "# define the number of samples per case\n",
    "n_samples = 100\n",
    "# define the number of lengths to test\n",
    "targ_lens = range(5, 101, 5); mses = []\n",
    "for targ_len in targ_lens:\n",
    "    # set seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    # track the MSEs\n",
    "    mse = 0\n",
    "    for sequence in np.random.choice(trbs4, size=n_samples, replace=False):\n",
    "        # retrieve the original length\n",
    "        orig_len = len(sequence)\n",
    "        # retrieve the embedding\n",
    "        embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "        tensor = torch.Tensor(embedding.T.reshape(1, 50, orig_len))\n",
    "        res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0].T\n",
    "        res_p = torch.nn.functional.interpolate(res.T.view((1, 50, targ_len)), size=(orig_len), mode='linear', align_corners=False)[0].T\n",
    "        mse += (res_p - embedding).pow(2).sum()\n",
    "    mses.append(torch.sqrt(mse / n_samples))\n",
    "# create the elbow like plot\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(targ_lens, mses, edgecolor='dodgerblue', color='skyblue', lw=1.5)\n",
    "ax.set(xlabel='TRB (4AA trimming) stretch length', ylabel='Average reconstruction loss')\n",
    "trb_mses = [x for x in mses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec233f-e0e6-4229-b340-482ffd96ab75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the number of lengths to test\n",
    "mses = []\n",
    "for targ_len in targ_lens:\n",
    "    # set seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    # track the MSEs\n",
    "    mse = 0\n",
    "    for sequence in np.random.choice(trbs3, size=n_samples, replace=False):\n",
    "        # retrieve the original length\n",
    "        orig_len = len(sequence)\n",
    "        # retrieve the embedding\n",
    "        embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "        tensor = torch.Tensor(embedding.T.reshape(1, 50, orig_len))\n",
    "        res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0].T\n",
    "        res_p = torch.nn.functional.interpolate(res.T.view((1, 50, targ_len)), size=(orig_len), mode='linear', align_corners=False)[0].T\n",
    "        mse += (res_p - embedding).pow(2).sum()\n",
    "    mses.append(torch.sqrt(mse / n_samples))\n",
    "# create the elbow like plot\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(targ_lens, mses, edgecolor='dodgerblue', color='skyblue', lw=1.5)\n",
    "ax.set(xlabel='TRB (3AA trimming) stretch length', ylabel='Average reconstruction loss')\n",
    "trb_mses = [x for x in mses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ef164-f2fd-45c7-bffe-7045a5254fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the number of lengths to test\n",
    "mses = []\n",
    "for targ_len in targ_lens:\n",
    "    # set seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    # track the MSEs\n",
    "    mse = 0\n",
    "    for sequence in np.random.choice(trbs2, size=n_samples, replace=False):\n",
    "        # retrieve the original length\n",
    "        orig_len = len(sequence)\n",
    "        # retrieve the embedding\n",
    "        embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "        tensor = torch.Tensor(embedding.T.reshape(1, 50, orig_len))\n",
    "        res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0].T\n",
    "        res_p = torch.nn.functional.interpolate(res.T.view((1, 50, targ_len)), size=(orig_len), mode='linear', align_corners=False)[0].T\n",
    "        mse += (res_p - embedding).pow(2).sum()\n",
    "    mses.append(torch.sqrt(mse / n_samples))\n",
    "# create the elbow like plot\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(targ_lens, mses, edgecolor='dodgerblue', color='skyblue', lw=1.5)\n",
    "ax.set(xlabel='TRB (2AA trimming) stretch length', ylabel='Average reconstruction loss')\n",
    "trb_mses = [x for x in mses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa95a3-f3b2-4838-ba1f-51d89844bf86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the number of lengths to test\n",
    "mses = []\n",
    "for targ_len in targ_lens:\n",
    "    # set seed for reproducibility\n",
    "    np.random.seed(0)\n",
    "    # track the MSEs\n",
    "    mse = 0\n",
    "    for sequence in np.random.choice(trbs1, size=n_samples, replace=False):\n",
    "        # retrieve the original length\n",
    "        orig_len = len(sequence)\n",
    "        # retrieve the embedding\n",
    "        embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "        tensor = torch.Tensor(embedding.T.reshape(1, 50, orig_len))\n",
    "        res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0].T\n",
    "        res_p = torch.nn.functional.interpolate(res.T.view((1, 50, targ_len)), size=(orig_len), mode='linear', align_corners=False)[0].T\n",
    "        mse += (res_p - embedding).pow(2).sum()\n",
    "    mses.append(torch.sqrt(mse / n_samples))\n",
    "# create the elbow like plot\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(targ_lens, mses, edgecolor='dodgerblue', color='skyblue', lw=1.5)\n",
    "ax.set(xlabel='TRB (1AA trimming) stretch length', ylabel='Average reconstruction loss')\n",
    "trb_mses = [x for x in mses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b870419-10f6-4bf4-bcc8-0718687d38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we therefore settle on a stretch length\n",
    "targ_len = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52f21f-bae0-4685-980d-598abb25d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# define a function to interpolate the protein\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    # add an the extra length information\n",
    "    res[-1, :] = orig_len\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a88b5f-cafa-45ae-a002-5f00229e5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs4):\n",
    "    # retrieve the embedding\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    # stretch the embedding\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    # save the embedding\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/map.trb_to_embed.extended_4aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf7c62-26eb-45b8-9c0a-468fec772186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs3):\n",
    "    # retrieve the embedding\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    # stretch the embedding\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    # save the embedding\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/map.trb_to_embed.extended_3aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6d47e-1d1f-4277-9b9b-b62c7da8ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs2):\n",
    "    # retrieve the embedding\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    # stretch the embedding\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    # save the embedding\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/map.trb_to_embed.extended_2aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71f136-55e8-4c3e-ae29-ccf37945b1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs1):\n",
    "    # retrieve the embedding\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    # stretch the embedding\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    # save the embedding\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/map.trb_to_embed.extended_1aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ed191-d988-4761-a12e-149049c82c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the TRBs\n",
    "with open('../outs/trbs.4aa.pkl', 'wb') as f: pkl.dump(trbs4, f)\n",
    "with open('../outs/trbs.3aa.pkl', 'wb') as f: pkl.dump(trbs3, f)\n",
    "with open('../outs/trbs.2aa.pkl', 'wb') as f: pkl.dump(trbs2, f)\n",
    "with open('../outs/trbs.1aa.pkl', 'wb') as f: pkl.dump(trbs1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b22be-d257-4182-b5f0-3d64d512625b",
   "metadata": {},
   "source": [
    "### Model Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0964d-362b-453c-85f1-c172e2ebbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0f5d8-014f-44a2-9375-5d7292f43457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (recon_batch, recon_len), mu, logvar = model(data)\n",
    "        if loss_func == 1:\n",
    "            loss = loss_function1(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 2:\n",
    "            loss = loss_function2(recon_batch, recon_len, data, mu, logvar)\n",
    "        elif loss_func == 3:\n",
    "            loss = loss_function3(recon_batch, recon_len, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "    \n",
    "def test(epoch, loss_func):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data[0].to(device)\n",
    "            (recon_batch, recon_len), mu, logvar = model(data)\n",
    "            if loss_func == 1:\n",
    "                test_loss += loss_function1(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 2:\n",
    "                test_loss += loss_function2(recon_batch, recon_len, data, mu, logvar).item()\n",
    "            elif loss_func == 3:\n",
    "                test_loss += loss_function3(recon_batch, recon_len, data, mu, logvar).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7100a-1e49-4a2a-87bc-1b0f4cfd20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the key parameters\n",
    "init_embed_size = 50-1\n",
    "protein_len = 48\n",
    "init_kernel_size = 3\n",
    "init_cnn_filters = 256\n",
    "init_kernel_stride = 1\n",
    "init_kernel_padding = 1\n",
    "secn_cnn_filters = 256\n",
    "latent_dim = 32\n",
    "vocab = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "# we want the embedding output to be the vocab with the length to allow for reconstruction\n",
    "out_embed_size = len(vocab)\n",
    "n_nodes_len = 32\n",
    "\n",
    "# define the convolutional variational autoencoder\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        # encoding\n",
    "        self.fc1 = nn.Conv1d(\n",
    "            in_channels=init_embed_size, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding,\n",
    "        )\n",
    "        self.fc2 = nn.Conv1d(\n",
    "            in_channels=init_cnn_filters, out_channels=secn_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        # variational sampling\n",
    "        self.fc31 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc32 = nn.Linear(secn_cnn_filters*protein_len, latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, secn_cnn_filters*protein_len)\n",
    "        # decoding\n",
    "        self.fc5 = nn.ConvTranspose1d(\n",
    "            in_channels=secn_cnn_filters, out_channels=init_cnn_filters, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc6 = nn.ConvTranspose1d(\n",
    "            in_channels=init_cnn_filters, out_channels=out_embed_size, kernel_size=init_kernel_size, \n",
    "            stride=init_kernel_stride, padding=init_kernel_padding\n",
    "        )\n",
    "        self.fc7 = nn.Linear(init_cnn_filters*protein_len, n_nodes_len)\n",
    "        self.fc8 = nn.Linear(n_nodes_len, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x1 = nn.LeakyReLU()(self.fc1(x[:, :-1, :]))\n",
    "        x2 = nn.LeakyReLU()(self.fc2(x1))\n",
    "        x2_ = nn.Flatten()(x2)\n",
    "        return self.fc31(x2_), self.fc32(x2_)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x4 = nn.LeakyReLU()(self.fc4(z))\n",
    "        x4_ = x4.view(-1, secn_cnn_filters, protein_len)\n",
    "        x5 = nn.LeakyReLU()(self.fc5(x4_))\n",
    "        x5_ = nn.Flatten()(x5)\n",
    "        x6 = nn.Sigmoid()(self.fc6(x5))\n",
    "        return x6, self.fc8(nn.LeakyReLU()(self.fc7(x5_)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3caf274-d93e-4e32-8905-8e605d25f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function1(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "def loss_function2(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return TSE\n",
    "def loss_function3(recon_x, recon_len, x, mu, logvar):\n",
    "    # get the data\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x[:, :len(vocab), :], reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    TSE = (recon_len - x[:, -1, :]).pow(2).sum()\n",
    "    return BCE + KLD + TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40402ed-05f2-4207-95bb-7d090a1e56d8",
   "metadata": {},
   "source": [
    "### TRB Trimming 4AAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a870e6-7899-45d3-af15-14a3c471ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../outs/trbs.4aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs))); print(len(trbs))\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended_4aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the trbs\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "\n",
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "    \n",
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "\n",
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')\n",
    "\n",
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "\n",
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae_4aa.trb.torch')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2)\n",
    "adata.obs_names = trbs\n",
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)\n",
    "\n",
    "# save the current data\n",
    "adata.write('../outs/adata.trb_4aa.h5ad')\n",
    "\n",
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res\n",
    "\n",
    "from Levenshtein import distance as levenshtein\n",
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=100, replace=False)\n",
    "# trbck distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(trbs_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10\n",
    "    \n",
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "print(ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']))\n",
    "print(ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbf2b8-eb1f-49c6-aaae-1e9bea93231f",
   "metadata": {},
   "source": [
    "### TRB Trimming 3AAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f85af5-dba5-4312-9379-d10b7d71977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../outs/trbs.3aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs))); print(len(trbs))\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended_3aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the trbs\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "\n",
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "    \n",
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "\n",
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')\n",
    "\n",
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "\n",
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae_3aa.trb.torch')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2)\n",
    "adata.obs_names = trbs\n",
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)\n",
    "\n",
    "# save the current data\n",
    "adata.write('../outs/adata.trb_3aa.h5ad')\n",
    "\n",
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res\n",
    "\n",
    "from Levenshtein import distance as levenshtein\n",
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=100, replace=False)\n",
    "# trbck distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(trbs_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10\n",
    "    \n",
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "print(ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']))\n",
    "print(ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884d8b9-69bd-461f-a8d0-2e5d105485ac",
   "metadata": {},
   "source": [
    "### TRB Trimming 2AAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d05f13-2439-4ca8-bb0b-1eb8dcdd5ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../outs/trbs.2aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs))); print(len(trbs))\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended_2aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the trbs\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "\n",
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "    \n",
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "\n",
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')\n",
    "\n",
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "\n",
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae_2aa.trb.torch')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2)\n",
    "adata.obs_names = trbs\n",
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)\n",
    "\n",
    "# save the current data\n",
    "adata.write('../outs/adata.trb_2aa.h5ad')\n",
    "\n",
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res\n",
    "\n",
    "from Levenshtein import distance as levenshtein\n",
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=100, replace=False)\n",
    "# trbck distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(trbs_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10\n",
    "    \n",
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "print(ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']))\n",
    "print(ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed3a6b-7a0a-4ba3-bf8d-0f58647c1165",
   "metadata": {},
   "source": [
    "### TRB Trimming 1AAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ef732-6a68-49ad-9c54-a2c4d331d7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../outs/trbs.1aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs))); print(len(trbs))\n",
    "# embed all of our unique TRBs\n",
    "with open('../outs/map.trb_to_embed.extended_1aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# randomly split the data into train and test\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs_train = np.random.choice(range(len(X_trbs)), size=round(len(X_trbs)*0.75), replace=False)\n",
    "idxs_test = np.array(range(len(X_trbs)))\n",
    "idxs_test = idxs_test[~np.isin(idxs_test, idxs_train)]\n",
    "X_trbs_train = X_trbs[idxs_train]\n",
    "X_trbs_test = X_trbs[idxs_test]\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# create a latent space for the trbs\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_trbs_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_trbs_test), batch_size=batch_size, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "\n",
    "# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# set the learning parameters\n",
    "lr = 0.0005; epochs = 20\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with dual losses to balance between two objectives\n",
    "train_losses = []; test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [1, 2]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "    \n",
    "# examine the KLD and BCE loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE Loss')\n",
    "# examine the length loss\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[1] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[1] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='TSE (LEN) Loss')# set the seed for training\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "# set the learning parameters\n",
    "lr = 0.001; epochs = 40\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# train the model with an integrated loss\n",
    "train_losses = []; test_losses = []; epochs\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_losses_, test_losses_ = [], []\n",
    "    for loss_func in [3]:\n",
    "        train_losses_.append(train(epoch, loss_func))\n",
    "        test_losses_.append(test(epoch, loss_func))\n",
    "    train_losses.append(train_losses_)\n",
    "    test_losses.append(test_losses_)\n",
    "\n",
    "# examine the integrated loss from finetuning\n",
    "fig, ax = plt.subplots(figsize=[4, 4]); ax.grid(False)\n",
    "ax.plot([x[0] for x in train_losses], color='dodgerblue')\n",
    "ax.plot([x[0] for x in test_losses], color='skyblue', linestyle='--')\n",
    "ax.set(xlabel='Epochs', ylabel='KLD + BCE + TSE Loss')\n",
    "\n",
    "# retrieve the predictions\n",
    "recon_lens = []; recon_batchs = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        data = data[0].to(device)\n",
    "        (recon_batch, recon_len), _, _ = model(data)\n",
    "        recon_batchs.extend(recon_batch.clone().detach().cpu().numpy())\n",
    "        recon_lens.extend(recon_len.clone().detach().cpu().tolist())\n",
    "recon_lens = [x[0] for x in recon_lens]\n",
    "# retrieve the indices\n",
    "trbs_test = pd.Series(trbs.iloc[idxs_test])\n",
    "\n",
    "# there is an ability to reconstruct proper length\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(recon_lens, trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Non-Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "# with rounding the picture becomes more clear, there is a linear relationship but not an exact one\n",
    "fig, ax = plt.subplots(); ax.grid(False)\n",
    "ax.scatter(pd.Series(recon_lens).apply(round), trbs_test.apply(len), s=10, alpha=0.1, zorder=1, color='dodgerblue')\n",
    "xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "domain = [min(min(xlim), min(ylim)), max(max(xlim), max(ylim))]\n",
    "ax.plot(domain, domain, color='skyblue', linestyle='--', zorder=0)\n",
    "ax.set_xlim(*xlim); ax.set_ylim(*ylim)\n",
    "ax.set(xlabel='Predicted length', ylabel='True length')\n",
    "print('Rounded', ss.pearsonr(recon_lens, trbs_test.apply(len)))\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), '../models/model.convvae_1aa.trb.torch')\n",
    "model.eval()\n",
    "\n",
    "from tqdm import tqdm\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "# create a complete loader or else there is an out of memory error\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "# move through each subset in the complete loader\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix\n",
    "adata = AnnData(z_dims, dtype=float)\n",
    "sc.tl.pca(adata)\n",
    "sc.pp.neighbors(adata, use_rep='X')\n",
    "sc.tl.umap(adata)\n",
    "sc.tl.leiden(adata, flavor='igraph', n_iterations=2)\n",
    "adata.obs_names = trbs\n",
    "# check length\n",
    "adata.obs['LEN'] = adata.obs.index.to_series().apply(len)\n",
    "sc.pl.umap(adata, color=['LEN'], vmin=12, vmax=20)\n",
    "\n",
    "# save the current data\n",
    "adata.write('../outs/adata.trb_1aa.h5ad')\n",
    "\n",
    "# define a function to embed an amino acid with vocab only\n",
    "map_direct = {x:[1 * (x == y) for y in vocab] for x in vocab}\n",
    "def embed_aa(aa):\n",
    "    return [x for x in map_direct[aa]]\n",
    "# define a function to interpolate the protein\n",
    "targ_len = 48\n",
    "def stretch_pep(embedding, targ_len=targ_len):\n",
    "    # get the current protein length\n",
    "    orig_len, n_features = embedding.shape\n",
    "    # derive the original and current lengths\n",
    "    x = np.linspace(0, 1, targ_len)\n",
    "    xp = np.linspace(0, 1, orig_len)\n",
    "    # loop through each of the columns\n",
    "    tensor = torch.Tensor(embedding.T.reshape(1, n_features, orig_len))\n",
    "    res = torch.nn.functional.interpolate(tensor, size=(targ_len), mode='linear', align_corners=False)[0]\n",
    "    return res\n",
    "\n",
    "from Levenshtein import distance as levenshtein\n",
    "# give examples if we had randomly chosen a \"true sequence\"\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "idxs = np.random.choice(range(len(trbs_test)), size=100, replace=False)\n",
    "# trbck distances\n",
    "df_dist = pd.DataFrame(columns=['dist_to_truth','dist_to_rand'])\n",
    "for idx in idxs:\n",
    "    # retrieve the sequence of interest\n",
    "    recon_batch = recon_batchs[idx]\n",
    "    recon_len = recon_lens[idx]\n",
    "    # retrieve the model resolved sequence and truth\n",
    "    pred = pd.DataFrame(recon_batch, index=vocab)\n",
    "    true_seq = trbs_test.iloc[idx]\n",
    "    true = pd.DataFrame(stretch_pep(np.array([embed for embed in map(embed_aa, list(true_seq))])).numpy(), index=vocab)\n",
    "    # resolve the sequence\n",
    "    curr_len = 48; targ_len = round(recon_len)\n",
    "    xp = np.arange(curr_len) / (curr_len - 1)\n",
    "    x = np.arange(targ_len) / (targ_len - 1)\n",
    "    # interpolate the results\n",
    "    res = np.array([np.interp(x, xp, recon_batch[idx, :]) for idx in range(recon_batch.shape[0])])\n",
    "    pred_seq = ''.join(pd.DataFrame(res.T, columns=vocab).idxmax(1))\n",
    "    dist_from_truth = levenshtein(true_seq, pred_seq); dist_from_rand = 0\n",
    "    for _ in range(10):\n",
    "        dist_from_rand += levenshtein(np.random.choice(trbs_test)[0], pred_seq)\n",
    "    df_dist.loc[df_dist.shape[0]] = dist_from_truth, dist_from_rand / 10\n",
    "    \n",
    "# compare prediction accuracy statistically\n",
    "fig, ax = plt.subplots(figsize=[2, 4]); ax.grid(False)\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='dodgerblue', order=['dist_to_truth'], color='skyblue')\n",
    "sns.boxplot(x='variable', y='value', data=df_dist.melt(), ax=ax, saturation=1,\n",
    "            linewidth=1.5, linecolor='grey', order=['dist_to_rand'], color='lightgray')\n",
    "# create artificial lines\n",
    "# for idx in df_dist.index: ax.plot([0, 1], df_dist.loc[idx], color='k', alpha=0.1, lw=0.5, zorder=0, linestyle='--')\n",
    "ax.tick_params(axis='x', labelrotation=90); ax.set_xticklabels(['Truth','Random']); ax.set_xlim(-1, 2)\n",
    "ax.set(xlabel='Prediction vs.', ylabel='Levenshtein distance')\n",
    "print(ss.wilcoxon(df_dist['dist_to_truth'], df_dist['dist_to_rand']))\n",
    "print(ss.mannwhitneyu(df_dist['dist_to_truth'], df_dist['dist_to_rand']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef4844-64a9-4c20-8e1f-02e6730ddd71",
   "metadata": {},
   "source": [
    "### COVID-19 Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a129d-3b60-4e94-8079-4556a87ccb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# read in the aggregated values\n",
    "with open('../external_data/dbv2.trbs.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(trbs)\n",
    "# trim the sequences\n",
    "trbs4 = [x[4:-4] for x in trbs if len(x[4:-4]) > 0]\n",
    "trbs3 = [x[3:-3] for x in trbs if len(x[3:-3]) > 0]\n",
    "trbs2 = [x[2:-2] for x in trbs if len(x[2:-2]) > 0]\n",
    "trbs1 = [x[1:-1] for x in trbs if len(x[1:-1]) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434aab50-d199-4f08-a559-d397805b49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs4):\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/mapv2.trb_to_embed.extended_4aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)\n",
    "\n",
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs3):\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/mapv2.trb_to_embed.extended_3aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)\n",
    "\n",
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs2):\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/mapv2.trb_to_embed.extended_2aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)\n",
    "\n",
    "# process the TRBs\n",
    "trb_to_embed = {}\n",
    "for sequence in tqdm(trbs1):\n",
    "    embedding = np.array([embed for embed in map(embed_aa, list(sequence))])\n",
    "    embedding = stretch_pep(embedding, targ_len=targ_len)\n",
    "    trb_to_embed[sequence] = embedding\n",
    "# save the embedding maps\n",
    "with open('../outs/mapv2.trb_to_embed.extended_1aatrim.pkl', 'wb') as f: pkl.dump(trb_to_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df8bac-9fa0-4da5-9817-91be3c3a9370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the TRBs\n",
    "with open('../outs/trbsv2.4aa.pkl', 'wb') as f: pkl.dump(trbs4, f)\n",
    "with open('../outs/trbsv2.3aa.pkl', 'wb') as f: pkl.dump(trbs3, f)\n",
    "with open('../outs/trbsv2.2aa.pkl', 'wb') as f: pkl.dump(trbs2, f)\n",
    "with open('../outs/trbsv2.1aa.pkl', 'wb') as f: pkl.dump(trbs1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84567418-9796-4b5f-8eeb-6ece6687de09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the aggregated values\n",
    "with open('../outs/trbsv2.1aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs)))\n",
    "with open('../outs/mapv2.trb_to_embed.extended_1aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "model.load_state_dict(torch.load('../models/model.convvae_1aa.trb.torch', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "pd.DataFrame(z_dims, index=trbs).to_csv('../outs/su22.trb_1aatrim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618acfa-8a27-44f3-8bd0-c7988a037894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated values\n",
    "with open('../outs/trbsv2.2aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs)))\n",
    "with open('../outs/mapv2.trb_to_embed.extended_2aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "model.load_state_dict(torch.load('../models/model.convvae_2aa.trb.torch', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "pd.DataFrame(z_dims, index=trbs).to_csv('../outs/su22.trb_2aatrim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33f46b-e4cf-40fd-9a77-e54e5e9a33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated values\n",
    "with open('../outs/trbsv2.3aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs)))\n",
    "with open('../outs/mapv2.trb_to_embed.extended_3aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "model.load_state_dict(torch.load('../models/model.convvae_3aa.trb.torch', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "pd.DataFrame(z_dims, index=trbs).to_csv('../outs/su22.trb_3aatrim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee157453-c997-4baf-83e3-e3d8f3162125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the aggregated values\n",
    "with open('../outs/trbsv2.4aa.pkl', 'rb') as f: trbs = pkl.load(f)\n",
    "trbs = pd.Series(list(set(trbs)))\n",
    "with open('../outs/mapv2.trb_to_embed.extended_4aatrim.pkl', 'rb') as f: trb_to_embed = pkl.load(f)\n",
    "X_trbs = torch.stack([x.to(torch.float32) for x in trbs.map(trb_to_embed)])\n",
    "# initialize the model\n",
    "model = ConvVAE().to(device)\n",
    "model.load_state_dict(torch.load('../models/model.convvae_4aa.trb.torch', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# get the encoded dimensions\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "complete_loader = DataLoader(dataset=TensorDataset(X_trbs), batch_size=batch_size, shuffle=False)\n",
    "z_dims_per_batch = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(complete_loader):\n",
    "        data = data[0].to(device)\n",
    "        enc_out = model.encode(data)\n",
    "        # sampling centers around the mean so we just use mu\n",
    "        z_dims_per_batch.append(enc_out[0].clone().detach().cpu().numpy())\n",
    "z_dims = np.vstack(z_dims_per_batch)\n",
    "del data, enc_out, z_dims_per_batch\n",
    "pd.DataFrame(z_dims, index=trbs).to_csv('../outs/su22.trb_4aatrim.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1a846-cac3-45e9-b860-1493c47bf687",
   "metadata": {},
   "source": [
    "### Gathering Trimmed Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef15a35-6387-4256-9fba-6842972788a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import auc, roc_curve, precision_recall_curve, f1_score, balanced_accuracy_score, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# read in the pickled data\n",
    "with open('../external_data/results.tcr.pkl', 'rb') as f:\n",
    "    results_tcr = pkl.load(f)\n",
    "# define the minimum number of cells\n",
    "min_cells = 2\n",
    "# derive annotations\n",
    "results_tcr['SU_CELL2022_COVID19'][['batch','subbatch','sample']] = \\\n",
    "results_tcr['SU_CELL2022_COVID19']['batch_info'].str.split(':', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7682e7-31d6-45b6-9465-11f7a85d5961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# define a function to interrogate the data\n",
    "def interrogate_with_globals():\n",
    "    # create statistics tracking dataframe\n",
    "    df_stat = pd.DataFrame(columns=['auroc','auprc','f1_score','balacc'])\n",
    "    # create tracking variables for downstream visualization and statistics\n",
    "    probas, probas_bin, truths = [], [], []\n",
    "    fprs, tprs, pres, recs = [], [], [], []\n",
    "    # train utilizing random forest models in a stratified shuffled manner\n",
    "    skf = StratifiedShuffleSplit(n_splits=10, random_state=0, test_size=1/4)\n",
    "    for idxs_train, idxs_test in skf.split(X1, y1):\n",
    "        # instantiate the random forest model\n",
    "        clf = LogisticRegression()\n",
    "        # fit the random forest model using Dataset #1\n",
    "        clf = clf.fit(X1.iloc[idxs_train], y1.iloc[idxs_train])\n",
    "\n",
    "        # predict on Dataset #2 correcting to all indices if requested\n",
    "        if pred_on_all:\n",
    "            idxs_test = range(X2.shape[0])\n",
    "        # derive the probabilities\n",
    "        proba = clf.predict_proba(X2.iloc[idxs_test])[:, clf.classes_ == 1]\n",
    "        probas.append(pd.Series(proba[:, 0], index=X2.index[idxs_test]))\n",
    "        # binarize into categorical predictions\n",
    "        proba_bin = 1 * (proba >= 0.50)\n",
    "        probas_bin.append(pd.Series(proba_bin[:, 0], index=X2.index[idxs_test]))\n",
    "        # retrieve the associated ground truth\n",
    "        truth = y2.iloc[idxs_test]\n",
    "        truths.append(truth.copy())\n",
    "\n",
    "        # compute subsequent AUROC and AUPRC related metrics\n",
    "        fpr, tpr, _ = roc_curve(truth, proba)\n",
    "        pre, rec, _ = precision_recall_curve(truth, proba)\n",
    "        fprs.append(fpr); tprs.append(tpr); pres.append(pre); recs.append(rec)\n",
    "        # save the relevant statistics\n",
    "        df_stat.loc[df_stat.shape[0]] = auc(fpr, tpr), auc(rec, pre), \\\n",
    "                                        f1_score(truth, proba_bin, average='binary'), \\\n",
    "                                        balanced_accuracy_score(truth, proba_bin)\n",
    "    return df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846368d7-3d46-471f-81d4-ffb735a7abd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop through each trim length\n",
    "for trim_len in range(1, 4+1, 1):\n",
    "    # read in the data\n",
    "    a_trb = sc.read_h5ad(f'../outs/adata.trb_{trim_len}aa.h5ad')\n",
    "    trb_covid = pd.read_csv(f'../outs/su22.trb_{trim_len}aatrim.csv', index_col=0)\n",
    "    # compile data\n",
    "    trbs_atlas = pd.DataFrame(a_trb.X, index=a_trb.obs.index)\n",
    "    trbs_atlas = trbs_atlas.loc[~trbs_atlas.index.isin(trb_covid.index)]\n",
    "    trb_covid.columns = trbs_atlas.columns\n",
    "    trbs_X = pd.concat([trbs_atlas, trb_covid], axis=0)\n",
    "    assert trbs_X.index.is_unique\n",
    "    \n",
    "    # get the tag, keeping only pairs that have at least min_cells cells\n",
    "    clusters = ['CD8+T','TREG','CD4+T']\n",
    "    mask = results_tcr['SUO_SCIENCE2022_FETAL']['celltype_annotation'].isin(clusters)\n",
    "    data = results_tcr['SUO_SCIENCE2022_FETAL'].loc[mask, ['donor','celltype_annotation','TRB']].astype(str).copy()\n",
    "    data['tag'] = data[['donor','celltype_annotation']].astype(str).agg(':'.join, axis=1)\n",
    "    data['TRB'] = data['TRB'].str.slice(trim_len, -trim_len)\n",
    "    # filter the data more harshly because less assured of quality\n",
    "    data['TRB'][~data['TRB'].isin(a_trb.obs.index)] = np.nan\n",
    "    data = data.dropna(subset=['TRB'])\n",
    "    counts = data['tag'].value_counts(); tags = counts.index[counts >= min_cells]\n",
    "    # compile the Xs\n",
    "    Xs = []\n",
    "    for tag in tqdm(tags):\n",
    "        trbs = data.loc[data['tag'] == tag, 'TRB']\n",
    "        mask = trbs[trbs.isin(a_trb.obs.index)]\n",
    "        X_ = pd.Series(a_trb[mask].X.mean(0), name=tag)\n",
    "        Xs.append(X_)\n",
    "    og_trb_suo2022_X = pd.concat(Xs, axis=1).T\n",
    "    \n",
    "    # get the tag, keeping only pairs that have at least min_cells cells\n",
    "    data = results_tcr['SU_CELL2022_COVID19'][['sample','TcellType','TRB']].astype(str).copy()\n",
    "    data['tag'] = data[['sample','TcellType']].astype(str).agg(':'.join, axis=1)\n",
    "    data['TRB'] = data['TRB'].str.slice(trim_len, -trim_len)\n",
    "    # filter the data more harshly because less assured of quality\n",
    "    data['TRB'][~data['TRB'].isin(trbs_X.index)] = np.nan\n",
    "    data = data.dropna(subset=['TRB'])\n",
    "    counts = data['tag'].value_counts(); tags = counts.index[counts >= min_cells]\n",
    "    # compile the Xs\n",
    "    Xs = []\n",
    "    for tag in tqdm(tags):\n",
    "        trbs = data.loc[data['tag'] == tag, 'TRB']\n",
    "        mask = trbs[trbs.isin(trbs_X.index)]\n",
    "        X_ = pd.Series(trbs_X.loc[mask].mean(0), name=tag)\n",
    "        Xs.append(X_)\n",
    "    og_trb_su2022_X = pd.concat(Xs, axis=1).T\n",
    "    \n",
    "    # get the tag, keeping only pairs that have at least min_cells cells\n",
    "    data = results_tcr['ZHENG_SCIENCE2021_PANCAN'][['patient','TcellType','TRB']].astype(str).copy()\n",
    "    data['tag'] = data[['patient','TcellType']].astype(str).agg(':'.join, axis=1)\n",
    "    data['TRB'] = data['TRB'].str.slice(trim_len, -trim_len)\n",
    "    # filter the data more harshly because less assured of quality\n",
    "    data['TRB'][~data['TRB'].isin(a_trb.obs.index)] = np.nan\n",
    "    data = data.dropna(subset=['TRB'])\n",
    "    counts = data['tag'].value_counts(); tags = counts.index[counts >= min_cells]\n",
    "    # compile the Xs\n",
    "    Xs = []\n",
    "    for tag in tqdm(tags):\n",
    "        trbs = data.loc[data['tag'] == tag, 'TRB']\n",
    "        mask = trbs[trbs.isin(a_trb.obs.index)]\n",
    "        X_ = pd.Series(a_trb[mask].X.mean(0), name=tag)\n",
    "        Xs.append(X_)\n",
    "    og_trb_zheng2021_X = pd.concat(Xs, axis=1).T\n",
    "    \n",
    "    # SELF PREDICTION\n",
    "    # define whether we are to predict on the complete data\n",
    "    pred_on_all = False\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_fetal = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_covid = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_tumor = interrogate_with_globals()\n",
    "    \n",
    "    # CROSS PREDICTION\n",
    "    # define whether we are to predict on the complete data\n",
    "    pred_on_all = True\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_fetal2tumor = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_fetal2covid = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    # > covid-19 and healthy donors\n",
    "    X2A = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2A = pd.Series(X2A.index.str.contains(':CD8'), index=X2A.index)\n",
    "    # > pan-cancer types\n",
    "    X2B = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2B = pd.Series(X2B.index.str.contains(':CD8'), index=X2B.index)\n",
    "    # > concatenate the two datasets\n",
    "    X2 = pd.concat([X2A, X2B], axis=0)\n",
    "    y2 = pd.concat([y2A, y2B], axis=0)\n",
    "    # perform predictions with all\n",
    "    df_stat_fetal2adult = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # reverse the comparison\n",
    "    X1, y1, X2, y2 = X2, y2, X1, y1\n",
    "    # perform predictions with all\n",
    "    df_stat_tumor2fetal = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # reverse the comparison\n",
    "    X1, y1, X2, y2 = X2, y2, X1, y1\n",
    "    # perform predictions with all\n",
    "    df_stat_covid2fetal = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_suo2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X1.index)\n",
    "    # define the data to predict on\n",
    "    # > covid-19 and healthy donors\n",
    "    X2A = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2A = pd.Series(X2A.index.str.contains(':CD8'), index=X2A.index)\n",
    "    # > pan-cancer types\n",
    "    X2B = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2B = pd.Series(X2B.index.str.contains(':CD8'), index=X2B.index)\n",
    "    # > concatenate the two datasets\n",
    "    X2 = pd.concat([X2A, X2B], axis=0)\n",
    "    y2 = pd.concat([y2A, y2B], axis=0)\n",
    "    # reverse the comparison\n",
    "    X1, y1, X2, y2 = X2, y2, X1, y1\n",
    "    # perform predictions with all\n",
    "    df_stat_adult2fetal = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X2A.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # perform predictions with all\n",
    "    df_stat_covid2tumor = interrogate_with_globals()\n",
    "    \n",
    "    # define the data to train on\n",
    "    X1 = og_trb_su2022_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y1 = pd.Series(X1.index.str.contains(':CD8'), index=X2A.index)\n",
    "    # define the data to predict on\n",
    "    X2 = og_trb_zheng2021_X.copy()\n",
    "    # setup a mask for CD8+ cells\n",
    "    y2 = pd.Series(X2.index.str.contains(':CD8'), index=X2.index)\n",
    "    # reverse the comparison\n",
    "    X1, y1, X2, y2 = X2, y2, X1, y1\n",
    "    # perform predictions with all\n",
    "    df_stat_tumor2covid = interrogate_with_globals()\n",
    "    \n",
    "    import pickle as pkl\n",
    "    # save all of the values\n",
    "    df_stats = {'adult2fetal':df_stat_adult2fetal,\n",
    "                'covid2fetal':df_stat_covid2fetal, 'covid2tumor':df_stat_covid2tumor, 'covid':df_stat_covid,\n",
    "                'tumor2covid':df_stat_tumor2covid, 'tumor':df_stat_tumor, 'tumor2fetal':df_stat_tumor2fetal,\n",
    "                'fetal2adult':df_stat_fetal2adult,\n",
    "                'fetal2covid':df_stat_fetal2covid, 'fetal2tumor':df_stat_fetal2tumor, 'fetal':df_stat_fetal,}\n",
    "    with open(f'../outs/250429v2_cd4vscd8_logisticregression.{trim_len}aa_stripped.pkl', 'wb') as f:\n",
    "        pkl.dump(df_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f74fc-8a37-4dfc-80a7-b6a9e40be8a1",
   "metadata": {},
   "source": [
    "#### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d7cb1-9612-4158-954f-67253b37aef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the stripped data, to compare with predictions of self\n",
    "h2d = {}\n",
    "with open('../outs/250429v2_cd4vscd8_logisticregression.4aa_stripped.pkl', 'rb') as f:\n",
    "    h2d['Trimmed by 4AA\\nOn Each Side'] = pkl.load(f)\n",
    "with open('../outs/250429v2_cd4vscd8_logisticregression.3aa_stripped.pkl', 'rb') as f:\n",
    "    h2d['Trimmed by 3AA\\nOn Each Side'] = pkl.load(f)\n",
    "with open('../outs/250429v2_cd4vscd8_logisticregression.2aa_stripped.pkl', 'rb') as f:\n",
    "    h2d['Trimmed by 2AA\\nOn Each Side'] = pkl.load(f)\n",
    "with open('../outs/250429v2_cd4vscd8_logisticregression.1aa_stripped.pkl', 'rb') as f:\n",
    "    h2d['Trimmed by 1AA\\nOn Each Side'] = pkl.load(f)\n",
    "# read in the non-stripped data, to compare with predictions of self\n",
    "with open('../outs/250421v2_cd4vscd8_logisticregression.pkl', 'rb') as f:\n",
    "    h2d['Untrimmed'] = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0a673-112a-4718-9a49-aca67d355fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a plotting function\n",
    "def visualize_on_globals():\n",
    "    # assemble the plotting dataframe\n",
    "    df_plot = pd.DataFrame(columns=['x','y','hue'])\n",
    "    for method, df_stats in h2d.items():\n",
    "        for k, vs in df_stats.items():\n",
    "            if '2' in k: continue\n",
    "            for v in vs[key]:\n",
    "                df_plot.loc[df_plot.shape[0]] = method, v, k\n",
    "\n",
    "    # create the ordered box plots\n",
    "    fig, ax = plt.subplots(figsize=[5, 4]); ax.grid(False)\n",
    "    order = list(h2d.keys())\n",
    "    sns.boxplot(x='x', y='y', hue='hue', data=df_plot, linewidth=1.5, linecolor='dodgerblue',\n",
    "                color='skyblue', saturation=1, showfliers=False, order=order)\n",
    "    sns.stripplot(x='x', y='y', hue='hue', data=df_plot, linewidth=1.5, edgecolor='dodgerblue',\n",
    "                  color='skyblue', jitter=0.25, order=order, s=5, alpha=0.5, dodge=True)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "    ax.axhline(0.5, color='grey', linestyle='--')\n",
    "    ax.legend(title=None, bbox_to_anchor=(1, .5), loc='center left', bbox_transform=ax.transAxes, frameon=False)\n",
    "    ax.set(xlabel='Datasets Utilized and Method', ylabel=key_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dfe47-1a6f-432b-a6a4-3e33b947aa28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = 'auroc'; key_label = 'AUROC'\n",
    "visualize_on_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12d61d-cbbf-4185-95ab-7bd110a0996e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = 'auprc'; key_label = 'AUPRC'\n",
    "visualize_on_globals()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu39",
   "language": "python",
   "name": "gpu39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
